"""
å®Œæ•´æ•°æ®å¤„ç†å·¥å…· - ä¼˜åŒ–ç‰ˆ
========================

ç¯å¢ƒè¦æ±‚:
- Python 3.8+
- streamlit >= 1.28.0
- pandas >= 1.5.0
- numpy >= 1.20.0
- openpyxl >= 3.0.0
- plotly >= 5.0.0

å®‰è£…å‘½ä»¤: pip install streamlit pandas numpy openpyxl plotly
"""

import streamlit as st
import pandas as pd
import datetime
import re
import io
import numpy as np
import warnings
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Optional, Tuple

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½æ•°æ®å¤„ç†å¹³å°",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
    }
    .feature-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
        border-left: 4px solid #667eea;
    }
    .metric-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        margin: 0.5rem 0;
    }
    .success-message {
        background: linear-gradient(90deg, #56ab2f 0%, #a8e6cf 100%);
        padding: 1rem;
        border-radius: 8px;
        color: white;
        margin: 1rem 0;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        background-color: #f0f2f6;
        border-radius: 8px 8px 0 0;
        padding: 0 20px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #667eea;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

def convert_date_to_sortable(date_str: str) -> str:
    """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ’åºçš„æ ¼å¼"""
    try:
        parts = date_str.split('/')
        if len(parts) == 3:
            year, month, day = parts
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{year}{month}{day}"
    except:
        pass
    return date_str

def force_standardize_date(date_str):
    """å¼ºåˆ¶æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œç¡®ä¿æœˆä»½å’Œæ—¥æœŸéƒ½æ˜¯ä¸¤ä½æ•°"""
    if pd.isna(date_str) or str(date_str).strip() == '':
        return date_str

    date_str = str(date_str).strip()

    # å¤„ç†ä¸åŒåˆ†éš”ç¬¦ï¼Œç»Ÿä¸€è½¬æ¢ä¸º/
    if '-' in date_str:
        date_str = date_str.replace('-', '/')

    # å¦‚æœåŒ…å«/ï¼Œåˆ™å¤„ç†
    if '/' in date_str:
        parts = date_str.split('/')
        if len(parts) == 3:
            try:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                # å¼ºåˆ¶æ ¼å¼åŒ–ä¸ºä¸¤ä½æ•°
                result = f"{year:04d}/{month:02d}/{day:02d}"
                return result
            except:
                pass

    return date_str

def create_data_visualization(df, title, chart_type="line"):
    """åˆ›å»ºæ•°æ®å¯è§†åŒ–å›¾è¡¨"""
    try:
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if df.empty:
            return None
            
        # è·å–æ—¥æœŸåˆ—ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—ï¼‰
        date_col = df.columns[0]
        
        # å°è¯•è½¬æ¢æ—¥æœŸæ ¼å¼
        try:
            df_viz = df.copy()
            df_viz[date_col] = pd.to_datetime(df_viz[date_col], errors='coerce')
            df_viz = df_viz.dropna(subset=[date_col])
            df_viz = df_viz.sort_values(date_col)
        except:
            df_viz = df.copy()
        
        # åˆ›å»ºå›¾è¡¨
        if chart_type == "line":
            # å¯»æ‰¾æ•°å€¼åˆ—
            numeric_cols = df_viz.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) > 0:
                fig = go.Figure()
                
                # æ·»åŠ å¤šä¸ªæ•°å€¼åˆ—çš„çº¿å›¾
                for col in numeric_cols[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡çº¿
                    fig.add_trace(go.Scatter(
                        x=df_viz[date_col],
                        y=df_viz[col],
                        mode='lines+markers',
                        name=col,
                        line=dict(width=2)
                    ))
                
                fig.update_layout(
                    title=title,
                    xaxis_title="æ—¥æœŸ",
                    yaxis_title="æ•°å€¼",
                    hovermode='x unified',
                    height=400,
                    template="plotly_white"
                )
                
                return fig
                
        elif chart_type == "bar":
            # åˆ›å»ºæŸ±çŠ¶å›¾
            if 'ä¸‰ç«¯' in df_viz.columns:
                # æŒ‰æ¸ é“åˆ†ç»„ç»Ÿè®¡
                numeric_cols = df_viz.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    agg_data = df_viz.groupby('ä¸‰ç«¯')[numeric_cols[0]].sum().reset_index()
                    
                    fig = px.bar(
                        agg_data,
                        x='ä¸‰ç«¯',
                        y=numeric_cols[0],
                        title=title,
                        color='ä¸‰ç«¯',
                        template="plotly_white"
                    )
                    
                    fig.update_layout(height=400)
                    return fig
        
        elif chart_type == "heatmap":
            # åˆ›å»ºçƒ­åŠ›å›¾ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„æ•°å€¼æ•°æ®ï¼‰
            numeric_cols = df_viz.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) >= 2:
                corr_matrix = df_viz[numeric_cols].corr()
                
                fig = px.imshow(
                    corr_matrix,
                    title=f"{title} - ç›¸å…³æ€§çƒ­åŠ›å›¾",
                    color_continuous_scale="RdBu_r",
                    aspect="auto"
                )
                
                fig.update_layout(height=400)
                return fig
                
    except Exception as e:
        st.warning(f"å›¾è¡¨åˆ›å»ºå¤±è´¥: {str(e)}")
        return None
    
    return None

def process_dau_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†DAUæ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    channel_dfs = {'mvp': [], 'and': [], 'ios': []}
    standard_columns = {'mvp': None, 'and': None, 'ios': None}
    
    processed_files = 0
    total_files = len(uploaded_files)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name
            
            if "dau" not in filename.lower():
                continue
            
            # ä»æ–‡ä»¶åä¸­æå–æ¸ é“ä¿¡æ¯
            if len(filename) > 7 and filename.startswith("dau_"):
                channel = filename[4:7]  # è·å–æ¸ é“å (mvp, and, ios)
                if channel not in channel_dfs:
                    continue
            else:
                continue
            
            # è¯»å–CSVæ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                try:
                    df = pd.read_csv(io.StringIO(content.decode('utf-8')), 
                                   na_values=[''], keep_default_na=False)
                except UnicodeDecodeError:
                    df = pd.read_csv(io.StringIO(content.decode('latin1')), 
                                   na_values=[''], keep_default_na=False)
                
            except Exception as e:
                continue
            
            if df.empty:
                continue
            
            # åˆ é™¤æŒ‡å®šçš„ä¸‰åˆ—
            columns_to_drop = ['Total Conversions', 'Re-attribution', 'Re-engagement']
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')
            
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            try:
                date_part = filename.split('_')[-1].replace('.csv', '')
                match = re.search(r'(\d+)\.(\d+)', date_part)
                if match:
                    month, day = match.groups()
                    formatted_date = f"2025/{month}/{day}"
                else:
                    formatted_date = date_part
            except:
                formatted_date = "2025/1/1"
            
            # æ·»åŠ æ—¥æœŸåˆ—
            df.insert(0, 'date', formatted_date)
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios' and 'Average eCPIUS$2.50' in df.columns:
                df = df.drop(columns=['Average eCPIUS$2.50'])
            
            # åˆ—æ ‡å‡†åŒ–
            if standard_columns[channel] is None:
                standard_columns[channel] = df.columns.tolist()
            else:
                current_cols = df.columns.tolist()
                if current_cols != standard_columns[channel]:
                    missing_cols = [col for col in standard_columns[channel] if col not in current_cols]
                    extra_cols = [col for col in current_cols if col not in standard_columns[channel]]
                    
                    if missing_cols:
                        for col in missing_cols:
                            df[col] = 'N/A'
                    
                    if extra_cols:
                        df = df.drop(columns=extra_cols)
                    
                    df = df[standard_columns[channel]]
            
            df = df.fillna('N/A')
            channel_dfs[channel].append(df)
            processed_files += 1
            
        except Exception as e:
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"DAUæ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶")
    
    if processed_files == 0:
        return None
    
    # åˆå¹¶æ•°æ®
    merged_by_channel = {}
    
    for channel, df_list in channel_dfs.items():
        if df_list:
            # ç¡®ä¿åˆ—ä¸€è‡´æ€§
            if len(df_list) > 1:
                standard_cols = list(df_list[0].columns)
                for i, df in enumerate(df_list):
                    if set(df.columns) != set(standard_cols):
                        missing = [col for col in standard_cols if col not in df.columns]
                        extra = [col for col in df.columns if col not in standard_cols]
                        
                        for col in missing:
                            df[col] = 'N/A'
                        if extra:
                            df = df.drop(columns=extra)
                        df = df[standard_cols]
                        df_list[i] = df
            
            merged_df = pd.concat(df_list, ignore_index=True)
            
            # æŒ‰æ—¥æœŸæ’åº
            try:
                merged_df['sort_key'] = merged_df['date'].apply(convert_date_to_sortable)
                merged_df = merged_df.sort_values(by='sort_key')
                merged_df = merged_df.drop(columns=['sort_key'])
            except Exception as e:
                pass
            
            merged_df = merged_df.fillna('N/A')
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios':
                expected_columns = ['date', 'Country', 'Impressions', 'Clicks', 'Installs', 'Conversion Rate',
                                  'Activity Sessions', 'Cost', 'Activity Revenue', 'Average eCPIUS$2.31',
                                  'Average DAU', 'Average MAU', 'Average DAU/MAU Rate', 'ARPDAU']
                
                extra_columns = [col for col in merged_df.columns if col not in expected_columns]
                if extra_columns:
                    merged_df = merged_df.drop(columns=extra_columns)
                
                missing_columns = [col for col in expected_columns if col not in merged_df.columns]
                if missing_columns:
                    for col in missing_columns:
                        merged_df[col] = 'N/A'
                
                merged_df = merged_df[expected_columns]
            
            merged_by_channel[channel] = merged_df
    
    return merged_by_channel

def process_retention_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†ç•™å­˜æ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    # æ¸ é“é…ç½®
    channels_config = {
        'ios': {
            'pattern': 'retention_ios.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'ios_formal': {
            'pattern': 'retention_ios_formal.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'mvp': {
            'pattern': 'retention_mvp.csv',
            'empty_columns': 1,
            'days': list(range(1, 8)) + [14]
        },
        'and': {
            'pattern': 'retention_and.csv',
            'empty_columns': 0,
            'days': list(range(1, 8)) + [14, 30]
        }
    }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    processed_data = {}
    total_files = len(uploaded_files)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name.lower()
            
            # åŒ¹é…æ¸ é“
            channel = None
            for ch, config in channels_config.items():
                if config['pattern'].lower() in filename:
                    channel = ch
                    break
            
            if not channel:
                continue
            
            # è¯»å–æ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(io.StringIO(content.decode(encoding)))
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df is None:
                    continue
                
            except Exception as e:
                continue
            
            # å¤„ç†æ—¥æœŸåˆ—
            date_column = "Cohort Day"
            if date_column not in df.columns:
                possible_date_columns = ['Date', 'date', 'æ—¥æœŸ', 'DAY', 'Day', 'day']
                for col in possible_date_columns:
                    if col in df.columns:
                        date_column = col
                        break
                
                if date_column not in df.columns:
                    continue
            
            # æ’åºæ•°æ®
            try:
                df[date_column] = pd.to_datetime(df[date_column])
                df = df.sort_values(by=date_column)
            except:
                try:
                    df = df.sort_values(by=date_column)
                except:
                    pass
            
            # æ£€æŸ¥ç”¨æˆ·åˆ—
            users_column = 'Users'
            if users_column not in df.columns:
                possible_users_columns = ['users', 'ç”¨æˆ·æ•°', 'DAU', 'User Count', 'user_count']
                for col in possible_users_columns:
                    if col in df.columns:
                        users_column = col
                        break
                
                if users_column not in df.columns:
                    continue
            
            # æ·»åŠ ç©ºåˆ—
            config = channels_config[channel]
            for j in range(config['empty_columns']):
                df[' ' * (j + 1)] = None
            
            # è®¡ç®—ç•™å­˜ç‡
            for day in config['days']:
                retention_column = f'sessions - Unique users - day {day} - partial'
                alternative_column = f'sessions - Unique users - day {day}'
                
                if retention_column in df.columns:
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
                elif alternative_column in df.columns:
                    df[retention_column] = df[alternative_column]
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
            
            processed_data[channel] = df
            
        except Exception as e:
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"ç•™å­˜æ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {len(processed_data)} ä¸ªæ–‡ä»¶")
    
    return processed_data if processed_data else None

def create_integrated_dau(merged_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆä¸‰ä¸ªæ¸ é“çš„DAUæ•°æ®"""
    if not merged_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    for channel, df in merged_data.items():
        df_copy = df.copy()
        # æ˜ å°„æ¸ é“å
        channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios'}
        df_copy.insert(1, 'ä¸‰ç«¯', channel_mapping.get(channel, channel))
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # ç»Ÿä¸€åˆ—
        all_columns = []
        for df in integrated_dfs:
            all_columns.extend(df.columns.tolist())
        
        unique_columns = list(dict.fromkeys(all_columns))
        
        standardized_dfs = []
        for df in integrated_dfs:
            df_copy = df.copy()
            
            for col in unique_columns:
                if col not in df_copy.columns:
                    df_copy[col] = 'N/A'
            
            df_copy = df_copy[unique_columns]
            standardized_dfs.append(df_copy)
        
        integrated_df = pd.concat(standardized_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if 'date' in integrated_df.columns:
            integrated_df['date'] = integrated_df['date'].apply(force_standardize_date)
        
        # æ’åº
        try:
            integrated_df['sort_key'] = integrated_df['date'].apply(convert_date_to_sortable)
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=['sort_key', 'æ¸ é“æ’åº'])
            integrated_df = integrated_df.drop(columns=['sort_key', 'æ¸ é“æ’åº'])
        except Exception as e:
            pass
        
        integrated_df = integrated_df.fillna('N/A')
        
        # åªä¿ç•™å‰15åˆ—åŠ ä¸‰ç«¯åˆ—
        if len(integrated_df.columns) > 16:
            first_15_cols = integrated_df.iloc[:, :15].columns.tolist()
            if "ä¸‰ç«¯" in integrated_df.columns and "ä¸‰ç«¯" not in first_15_cols:
                cols_to_keep = first_15_cols + ["ä¸‰ç«¯"]
                integrated_df = integrated_df[cols_to_keep]
        
        return integrated_df
        
    except Exception as e:
        return pd.DataFrame()

def create_integrated_retention(retention_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆç•™å­˜æ•°æ®"""
    if not retention_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    # æ¸ é“æ˜ å°„
    channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios', 'ios_formal': 'ios'}
    
    # æœŸæœ›çš„åˆ—é¡ºåº
    expected_columns = [
        "Cohort Day", "Ltv Country", "Campaign Id", "Keywords", "Users", "Cost", "Average eCPI",
        "sessions - Unique users - day 1 - partial", "sessions - Unique users - day 2 - partial",
        "sessions - Unique users - day 3 - partial", "sessions - Unique users - day 4 - partial",
        "sessions - Unique users - day 5 - partial", "sessions - Unique users - day 6 - partial",
        "sessions - Unique users - day 7 - partial", "sessions - Unique users - day 14 - partial",
        "sessions - Unique users - day 30 - partial", "Unnamed: 16", "Unnamed: 17", "Unnamed: 18",
        "day1", "day2", "day3", "day4", "day5", "day6", "day7", "day14", "day30",
        "ä¸‰ç«¯"
    ]
    
    for channel, df in retention_data.items():
        df_copy = df.copy()
        
        # æ˜ å°„æ¸ é“å
        mapped_channel = channel_mapping.get(channel, channel)
        df_copy["ä¸‰ç«¯"] = mapped_channel
        
        # åˆ—åä¿®æ­£
        if channel in ["mvp", "and"]:
            alt_day14 = "sessions - Unique users - day 14- partial"
            std_day14 = "sessions - Unique users - day 14 - partial"
            if alt_day14 in df_copy.columns and std_day14 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day14: std_day14})
            
            alt_day30 = "sessions - Unique users - day 30- partial"
            std_day30 = "sessions - Unique users - day 30 - partial"
            if alt_day30 in df_copy.columns and std_day30 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day30: std_day30})
        
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # åˆå¹¶æ•°æ®
        integrated_df = pd.concat(integrated_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if "Cohort Day" in integrated_df.columns:
            integrated_df["Cohort Day"] = integrated_df["Cohort Day"].apply(force_standardize_date)
        
        # é‡æ–°æ’åºåˆ—å¹¶å¡«å……ç¼ºå¤±åˆ—
        final_columns = []
        for col in expected_columns:
            if col not in integrated_df.columns:
                integrated_df[col] = None
            final_columns.append(col)
        
        existing_cols = [col for col in final_columns if col in integrated_df.columns]
        integrated_df = integrated_df[existing_cols]
        
        # æ¸…ç©ºUnnamedåˆ—
        unnamed_cols = [col for col in integrated_df.columns if 'Unnamed' in col]
        for col in unnamed_cols:
            integrated_df[col] = ''
        
        # æ’åº
        try:
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=["Cohort Day", "æ¸ é“æ’åº"])
            integrated_df = integrated_df.drop(columns=["æ¸ é“æ’åº"])
        except Exception as e:
            pass
        
        integrated_df = integrated_df.fillna('N/A')
        
        return integrated_df
        
    except Exception as e:
        return pd.DataFrame()

def main():
    # ä¸»æ ‡é¢˜
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ æ™ºèƒ½æ•°æ®å¤„ç†å¹³å°</h1>
        <p>DAUåˆå¹¶ â€¢ ç•™å­˜è®¡ç®— â€¢ æ•°æ®å¯è§†åŒ– â€¢ ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æ ¸å¿ƒåŠŸèƒ½åŒºåŸŸ
    st.markdown("## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½")
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ“ˆ DAUæ•°æ®å¤„ç†</h3>
            <p>â€¢ è‡ªåŠ¨è¯†åˆ«æ¸ é“æ–‡ä»¶<br>
            â€¢ æ™ºèƒ½æ•°æ®åˆå¹¶<br>
            â€¢ å®æ—¶å¯è§†åŒ–åˆ†æ</p>
        </div>
        """, unsafe_allow_html=True)
        
        # DAUæ–‡ä»¶ä¸Šä¼ 
        dau_files = st.file_uploader(
            "æ‹–å…¥DAUæ–‡ä»¶ (æ”¯æŒå¤šé€‰)",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶æ ¼å¼: dau_æ¸ é“_æ—¥æœŸ.csv",
            key="dau_uploader"
        )
        
        if dau_files:
            st.success(f"âœ… å·²é€‰æ‹© {len(dau_files)} ä¸ªDAUæ–‡ä»¶")
            
            if st.button("ğŸš€ å¼€å§‹å¤„ç†DAUæ•°æ®", type="primary", key="process_dau"):
                with st.spinner("ğŸ”„ æ™ºèƒ½å¤„ç†ä¸­..."):
                    st.session_state.dau_results = process_dau_files(dau_files)
                
                if st.session_state.dau_results:
                    st.markdown("""
                    <div class="success-message">
                        âœ¨ DAUæ•°æ®å¤„ç†å®Œæˆï¼å·²ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œåˆ†ææŠ¥å‘Š
                    </div>
                    """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="feature-card">
            <h3>ğŸ”„ ç•™å­˜æ•°æ®å¤„ç†</h3>
            <p>â€¢ è‡ªåŠ¨è®¡ç®—ç•™å­˜ç‡<br>
            â€¢ å¤šæ¸ é“æ•°æ®æ•´åˆ<br>
            â€¢ è¶‹åŠ¿åˆ†æå¯è§†åŒ–</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ç•™å­˜æ–‡ä»¶ä¸Šä¼ 
        retention_files = st.file_uploader(
            "æ‹–å…¥ç•™å­˜æ–‡ä»¶ (æ”¯æŒå¤šé€‰)",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶æ ¼å¼: retention_æ¸ é“.csv",
            key="retention_uploader"
        )
        
        if retention_files:
            st.success(f"âœ… å·²é€‰æ‹© {len(retention_files)} ä¸ªç•™å­˜æ–‡ä»¶")
            
            if st.button("ğŸš€ å¼€å§‹å¤„ç†ç•™å­˜æ•°æ®", type="primary", key="process_retention"):
                with st.spinner("ğŸ”„ æ™ºèƒ½å¤„ç†ä¸­..."):
                    st.session_state.retention_results = process_retention_files(retention_files)
                
                if st.session_state.retention_results:
                    st.markdown("""
                    <div class="success-message">
                        âœ¨ ç•™å­˜æ•°æ®å¤„ç†å®Œæˆï¼å·²ç”Ÿæˆç•™å­˜ç‡åˆ†æå’Œè¶‹åŠ¿å›¾è¡¨
                    </div>
                    """, unsafe_allow_html=True)
    
    # æ•°æ®å¯è§†åŒ–å’Œç»“æœå±•ç¤º
    if 'dau_results' in st.session_state and st.session_state.dau_results:
        st.markdown("---")
        st.markdown("## ğŸ“Š DAUæ•°æ®åˆ†æ")
        
        # åˆ›å»ºæ•´åˆæ•°æ®
        integrated_dau = create_integrated_dau(st.session_state.dau_results)
        
        if not integrated_dau.empty:
            # æ•°æ®ç»Ÿè®¡å¡ç‰‡
            metric_cols = st.columns(4)
            with metric_cols[0]:
                st.markdown("""
                <div class="metric-card">
                    <h3>{}</h3>
                    <p>å¤„ç†æ¸ é“æ•°</p>
                </div>
                """.format(len(st.session_state.dau_results)), unsafe_allow_html=True)
            
            with metric_cols[1]:
                total_rows = sum(len(df) for df in st.session_state.dau_results.values())
                st.markdown("""
                <div class="metric-card">
                    <h3>{}</h3>
                    <p>æ€»æ•°æ®è¡Œæ•°</p>
                </div>
                """.format(total_rows), unsafe_allow_html=True)
            
            with metric_cols[2]:
                st.markdown("""
                <div class="metric-card">
                    <h3>{}</h3>
                    <p>æ•´åˆåè¡Œæ•°</p>
                </div>
                """.format(len(integrated_dau)), unsafe_allow_html=True)
            
            with metric_cols[3]:
                if 'Installs' in integrated_dau.columns:
                    total_installs = integrated_dau['Installs'].replace('N/A', 0).astype(str).str.replace(',', '').astype(float).sum()
                    st.markdown("""
                    <div class="metric-card">
                        <h3>{:,.0f}</h3>
                        <p>æ€»å®‰è£…é‡</p>
                    </div>
                    """.format(total_installs), unsafe_allow_html=True)
            
            # å¯è§†åŒ–å›¾è¡¨
            viz_cols = st.columns(2)
            
            with viz_cols[0]:
                # è¶‹åŠ¿å›¾
                fig_line = create_data_visualization(integrated_dau, "DAUæ•°æ®è¶‹åŠ¿åˆ†æ", "line")
                if fig_line:
                    st.plotly_chart(fig_line, use_container_width=True)
            
            with viz_cols[1]:
                # æ¸ é“å¯¹æ¯”å›¾
                fig_bar = create_data_visualization(integrated_dau, "æ¸ é“æ•°æ®å¯¹æ¯”", "bar")
                if fig_bar:
                    st.plotly_chart(fig_bar, use_container_width=True)
            
            # æ•°æ®é¢„è§ˆ
            st.markdown("### ğŸ“‹ æ•°æ®é¢„è§ˆ")
            preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯DAUæ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in st.session_state.dau_results.keys()])
            
            with preview_tabs[0]:
                st.dataframe(integrated_dau.head(15), use_container_width=True)
            
            for i, (channel, df) in enumerate(st.session_state.dau_results.items()):
                with preview_tabs[i + 1]:
                    st.dataframe(df.head(15), use_container_width=True)
            
            # ä¸‹è½½æŒ‰é’®
            st.markdown("### ğŸ’¾ ä¸‹è½½æ•°æ®")
            download_cols = st.columns(3)
            
            today = datetime.datetime.now().strftime("%m.%d")
            
            with download_cols[0]:
                csv_data = integrated_dau.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ä¸‰ç«¯DAUæ±‡æ€»",
                    data=csv_data.encode('utf-8-sig'),
                    file_name=f"{today} ä¸‰ç«¯dauæ±‡æ€».csv",
                    mime="text/csv",
                    type="primary"
                )
            
            # åˆ†æ¸ é“ä¸‹è½½
            for i, (channel, df) in enumerate(st.session_state.dau_results.items()):
                col_idx = (i + 1) % 3
                with download_cols[col_idx]:
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ“¥ {channel.upper()}æ¸ é“",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} dau_{channel}.csv",
                        mime="text/csv",
                        key=f"dau_{channel}"
                    )
    
    # ç•™å­˜æ•°æ®å¯è§†åŒ–
    if 'retention_results' in st.session_state and st.session_state.retention_results:
        st.markdown("---")
        st.markdown("## ğŸ”„ ç•™å­˜æ•°æ®åˆ†æ")
        
        # åˆ›å»ºæ•´åˆæ•°æ®
        integrated_retention = create_integrated_retention(st.session_state.retention_results)
        
        if not integrated_retention.empty:
            # ç•™å­˜ç‡è¶‹åŠ¿å›¾
            retention_viz_cols = st.columns(2)
            
            with retention_viz_cols[0]:
                # ç•™å­˜ç‡è¶‹åŠ¿
                fig_retention = create_data_visualization(integrated_retention, "ç•™å­˜ç‡è¶‹åŠ¿åˆ†æ", "line")
                if fig_retention:
                    st.plotly_chart(fig_retention, use_container_width=True)
            
            with retention_viz_cols[1]:
                # æ¸ é“ç•™å­˜å¯¹æ¯”
                fig_retention_bar = create_data_visualization(integrated_retention, "æ¸ é“ç•™å­˜å¯¹æ¯”", "bar")
                if fig_retention_bar:
                    st.plotly_chart(fig_retention_bar, use_container_width=True)
            
            # ç•™å­˜æ•°æ®é¢„è§ˆ
            st.markdown("### ğŸ“‹ ç•™å­˜æ•°æ®é¢„è§ˆ")
            retention_preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯ç•™å­˜æ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in st.session_state.retention_results.keys()])
            
            with retention_preview_tabs[0]:
                st.dataframe(integrated_retention.head(15), use_container_width=True)
            
            for i, (channel, df) in enumerate(st.session_state.retention_results.items()):
                with retention_preview_tabs[i + 1]:
                    st.dataframe(df.head(15), use_container_width=True)
            
            # ç•™å­˜æ•°æ®ä¸‹è½½
            st.markdown("### ğŸ’¾ ä¸‹è½½ç•™å­˜æ•°æ®")
            retention_download_cols = st.columns(3)
            
            with retention_download_cols[0]:
                csv_data = integrated_retention.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ä¸‰ç«¯ç•™å­˜æ±‡æ€»",
                    data=csv_data.encode('utf-8-sig'),
                    file_name=f"{today} ä¸‰ç«¯ç•™å­˜æ±‡æ€».csv",
                    mime="text/csv",
                    type="primary",
                    key="retention_integrated"
                )
            
            # åˆ†æ¸ é“ä¸‹è½½
            for i, (channel, df) in enumerate(st.session_state.retention_results.items()):
                col_idx = (i + 1) % 3
                with retention_download_cols[col_idx]:
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ“¥ {channel.upper()}ç•™å­˜",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ç•™å­˜_{channel}.csv",
                        mime="text/csv",
                        key=f"retention_{channel}"
                    )
    
    # é«˜çº§åŠŸèƒ½åŒºåŸŸ
    if not ('dau_results' in st.session_state and st.session_state.dau_results) and not ('retention_results' in st.session_state and st.session_state.retention_results):
        st.markdown("---")
        st.markdown("## ğŸ¯ å¼€å§‹ä½¿ç”¨")
        st.info("ğŸ‘† è¯·åœ¨ä¸Šæ–¹æ‹–å…¥æ‚¨çš„æ•°æ®æ–‡ä»¶å¼€å§‹å¤„ç†ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–åˆ†ææŠ¥å‘Š")
    
    # åŠŸèƒ½è¯´æ˜ï¼ˆæŠ˜å æ˜¾ç¤ºï¼‰
    with st.expander("ğŸ“‹ è¯¦ç»†åŠŸèƒ½è¯´æ˜", expanded=False):
        st.markdown("""
        ### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
        
        **ğŸ“ˆ DAUæ•°æ®å¤„ç†**
        - è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶æ ¼å¼ï¼š`dau_æ¸ é“_æ—¥æœŸ.csv`
        - æ”¯æŒæ¸ é“ï¼šMVPã€Androidã€iOS
        - æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        - è‡ªåŠ¨ç”Ÿæˆè¶‹åŠ¿åˆ†æå›¾è¡¨
        
        **ğŸ”„ ç•™å­˜æ•°æ®å¤„ç†**
        - è‡ªåŠ¨è®¡ç®—1-7æ—¥ã€14æ—¥ã€30æ—¥ç•™å­˜ç‡
        - æ”¯æŒå¤šæ¸ é“æ•°æ®æ•´åˆ
        - ç”Ÿæˆç•™å­˜ç‡è¶‹åŠ¿å¯è§†åŒ–
        - æ¸ é“é—´å¯¹æ¯”åˆ†æ
        
        **ğŸ“Š æ•°æ®å¯è§†åŒ–**
        - å®æ—¶ç”Ÿæˆäº¤äº’å¼å›¾è¡¨
        - å¤šç»´åº¦æ•°æ®åˆ†æ
        - è¶‹åŠ¿é¢„æµ‹å’Œæ´å¯Ÿ
        - æ”¯æŒæ•°æ®å¯¼å‡º
        
        ### ğŸ“ æ–‡ä»¶æ ¼å¼è¦æ±‚
        
        **DAUæ–‡ä»¶**ï¼š`dau_mvp_3.17.csv`ã€`dau_and_3.18.csv`ã€`dau_ios_3.19.csv`
        
        **ç•™å­˜æ–‡ä»¶**ï¼š`retention_mvp.csv`ã€`retention_and.csv`ã€`retention_ios.csv`
        
        ### ğŸš€ å¿«é€Ÿå¼€å§‹
        1. æ‹–å…¥æ–‡ä»¶åˆ°å¯¹åº”ä¸Šä¼ åŒºåŸŸ
        2. ç‚¹å‡»"å¼€å§‹å¤„ç†"æŒ‰é’®
        3. æŸ¥çœ‹è‡ªåŠ¨ç”Ÿæˆçš„å¯è§†åŒ–åˆ†æ
        4. ä¸‹è½½å¤„ç†åçš„æ•°æ®æ–‡ä»¶
        
        ### ğŸ’¡ æŠ€æœ¯ç‰¹æ€§
        - æ™ºèƒ½ç¼–ç è¯†åˆ«ï¼Œæ”¯æŒä¸­æ–‡æ–‡ä»¶
        - è‡ªåŠ¨æ•°æ®ç±»å‹æ¨æ–­
        - å¼‚å¸¸æ•°æ®å¤„ç†å’Œä¿®å¤
        - é«˜æ€§èƒ½æ•°æ®å¤„ç†å¼•æ“
        """)
    
    # é¡µè„š
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 2rem;'>
        <p><strong>ğŸš€ æ™ºèƒ½æ•°æ®å¤„ç†å¹³å°</strong> | è®©æ•°æ®åˆ†ææ›´ç®€å•é«˜æ•ˆ</p>
        <small>æ”¯æŒDAUåˆå¹¶ â€¢ ç•™å­˜è®¡ç®— â€¢ æ•°æ®å¯è§†åŒ– â€¢ ä¸€é”®å¯¼å‡º</small>
    </div>
    """, unsafe_allow_html=True)

# åˆå§‹åŒ–session state
if 'dau_results' not in st.session_state:
    st.session_state.dau_results = None
if 'retention_results' not in st.session_state:
    st.session_state.retention_results = None

if __name__ == "__main__":
    main()
