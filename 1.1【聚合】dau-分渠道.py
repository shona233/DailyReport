import streamlit as st
import pandas as pd
import datetime
import re
import io
from typing import Dict, List, Optional, Tuple

def convert_date_to_sortable(date_str: str) -> str:
    """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ’åºçš„æ ¼å¼"""
    try:
        parts = date_str.split('/')
        if len(parts) == 3:
            year, month, day = parts
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{year}{month}{day}"
    except:
        pass
    return date_str

def force_standardize_date(date_str):
    """å¼ºåˆ¶æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œç¡®ä¿æœˆä»½å’Œæ—¥æœŸéƒ½æ˜¯ä¸¤ä½æ•°"""
    if pd.isna(date_str) or str(date_str).strip() == '':
        return date_str

    date_str = str(date_str).strip()

    # å¤„ç†ä¸åŒåˆ†éš”ç¬¦ï¼Œç»Ÿä¸€è½¬æ¢ä¸º/
    if '-' in date_str:
        date_str = date_str.replace('-', '/')

    # å¦‚æœåŒ…å«/ï¼Œåˆ™å¤„ç†
    if '/' in date_str:
        parts = date_str.split('/')
        if len(parts) == 3:
            try:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                # å¼ºåˆ¶æ ¼å¼åŒ–ä¸ºä¸¤ä½æ•°
                result = f"{year:04d}/{month:02d}/{day:02d}"
                return result
            except:
                pass

    return date_str

def process_dau_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†DAUæ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    channel_dfs = {'mvp': [], 'and': [], 'ios': []}
    standard_columns = {'mvp': None, 'and': None, 'ios': None}
    
    processed_files = 0
    total_files = len(uploaded_files)
    
    log_container = st.expander("DAUæ–‡ä»¶å¤„ç†è¯¦æƒ…", expanded=False)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name
            
            if "dau" not in filename.lower():
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åä¸åŒ…å«'dau'")
                continue
            
            # ä»æ–‡ä»¶åä¸­æå–æ¸ é“ä¿¡æ¯
            if len(filename) > 7 and filename.startswith("dau_"):
                channel = filename[4:7]  # è·å–æ¸ é“å (mvp, and, ios)
                if channel not in channel_dfs:
                    log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ— æ³•è¯†åˆ«çš„æ¸ é“ '{channel}'")
                    continue
            else:
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                continue
            
            # è¯»å–CSVæ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                try:
                    df = pd.read_csv(io.StringIO(content.decode('utf-8')), 
                                   na_values=[''], keep_default_na=False)
                except UnicodeDecodeError:
                    df = pd.read_csv(io.StringIO(content.decode('latin1')), 
                                   na_values=[''], keep_default_na=False)
                
                log_container.success(f"æˆåŠŸè¯»å– {filename}, å½¢çŠ¶: {df.shape}")
                
            except Exception as e:
                log_container.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
                continue
            
            if df.empty:
                log_container.warning(f"æ–‡ä»¶ {filename} ä¸åŒ…å«æ•°æ®ï¼Œå·²è·³è¿‡")
                continue
            
            # åˆ é™¤æŒ‡å®šçš„ä¸‰åˆ—
            columns_to_drop = ['Total Conversions', 'Re-attribution', 'Re-engagement']
            original_cols = df.columns.tolist()
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')
            removed_cols = [col for col in columns_to_drop if col in original_cols]
            if removed_cols:
                log_container.info(f"å·²åˆ é™¤åˆ—: {', '.join(removed_cols)}")
            
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            try:
                date_part = filename.split('_')[-1].replace('.csv', '')
                match = re.search(r'(\d+)\.(\d+)', date_part)
                if match:
                    month, day = match.groups()
                    formatted_date = f"2025/{month}/{day}"
                else:
                    formatted_date = date_part
            except:
                formatted_date = "2025/1/1"
                log_container.warning(f"æ— æ³•ä»æ–‡ä»¶å {filename} æå–æ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            # æ·»åŠ æ—¥æœŸåˆ—
            df.insert(0, 'date', formatted_date)
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios' and 'Average eCPIUS$2.50' in df.columns:
                df = df.drop(columns=['Average eCPIUS$2.50'])
                log_container.info(f"ç§»é™¤iOSä¸­çš„é—®é¢˜åˆ—")
            
            # åˆ—æ ‡å‡†åŒ–
            if standard_columns[channel] is None:
                standard_columns[channel] = df.columns.tolist()
            else:
                current_cols = df.columns.tolist()
                if current_cols != standard_columns[channel]:
                    missing_cols = [col for col in standard_columns[channel] if col not in current_cols]
                    extra_cols = [col for col in current_cols if col not in standard_columns[channel]]
                    
                    if missing_cols:
                        for col in missing_cols:
                            df[col] = 'N/A'
                    
                    if extra_cols:
                        df = df.drop(columns=extra_cols)
                    
                    df = df[standard_columns[channel]]
            
            df = df.fillna('N/A')
            channel_dfs[channel].append(df)
            processed_files += 1
            
        except Exception as e:
            log_container.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"DAUæ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶")
    
    if processed_files == 0:
        st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•DAUæ–‡ä»¶")
        return None
    
    # åˆå¹¶æ•°æ®
    merged_by_channel = {}
    
    for channel, df_list in channel_dfs.items():
        if df_list:
            # ç¡®ä¿åˆ—ä¸€è‡´æ€§
            if len(df_list) > 1:
                standard_cols = list(df_list[0].columns)
                for i, df in enumerate(df_list):
                    if set(df.columns) != set(standard_cols):
                        missing = [col for col in standard_cols if col not in df.columns]
                        extra = [col for col in df.columns if col not in standard_cols]
                        
                        for col in missing:
                            df[col] = 'N/A'
                        if extra:
                            df = df.drop(columns=extra)
                        df = df[standard_cols]
                        df_list[i] = df
            
            merged_df = pd.concat(df_list, ignore_index=True)
            
            # æŒ‰æ—¥æœŸæ’åº
            try:
                merged_df['sort_key'] = merged_df['date'].apply(convert_date_to_sortable)
                merged_df = merged_df.sort_values(by='sort_key')
                merged_df = merged_df.drop(columns=['sort_key'])
            except Exception as e:
                st.warning(f"æ¸ é“ {channel} æ’åºæ—¶å‡ºé”™: {str(e)}")
            
            merged_df = merged_df.fillna('N/A')
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios':
                expected_columns = ['date', 'Country', 'Impressions', 'Clicks', 'Installs', 'Conversion Rate',
                                  'Activity Sessions', 'Cost', 'Activity Revenue', 'Average eCPIUS$2.31',
                                  'Average DAU', 'Average MAU', 'Average DAU/MAU Rate', 'ARPDAU']
                
                extra_columns = [col for col in merged_df.columns if col not in expected_columns]
                if extra_columns:
                    merged_df = merged_df.drop(columns=extra_columns)
                
                missing_columns = [col for col in expected_columns if col not in merged_df.columns]
                if missing_columns:
                    for col in missing_columns:
                        merged_df[col] = 'N/A'
                
                merged_df = merged_df[expected_columns]
            
            merged_by_channel[channel] = merged_df
    
    return merged_by_channel

def process_retention_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†ç•™å­˜æ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    # æ¸ é“é…ç½®
    channels_config = {
        'ios': {
            'pattern': 'retention_ios.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'ios_formal': {
            'pattern': 'retention_ios_formal.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'mvp': {
            'pattern': 'retention_mvp.csv',
            'empty_columns': 1,
            'days': list(range(1, 8)) + [14]
        },
        'and': {
            'pattern': 'retention_and.csv',
            'empty_columns': 0,
            'days': list(range(1, 8)) + [14, 30]
        }
    }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("ç•™å­˜æ–‡ä»¶å¤„ç†è¯¦æƒ…", expanded=False)
    
    processed_data = {}
    total_files = len(uploaded_files)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name.lower()
            
            # åŒ¹é…æ¸ é“
            channel = None
            for ch, config in channels_config.items():
                if config['pattern'].lower() in filename:
                    channel = ch
                    break
            
            if not channel:
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {uploaded_file.name}: æ— æ³•è¯†åˆ«çš„ç•™å­˜æ–‡ä»¶")
                continue
            
            # è¯»å–æ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(io.StringIO(content.decode(encoding)))
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df is None:
                    log_container.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {uploaded_file.name}")
                    continue
                
                log_container.success(f"æˆåŠŸè¯»å– {uploaded_file.name}, å½¢çŠ¶: {df.shape}")
                
            except Exception as e:
                log_container.error(f"è¯»å–æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
                continue
            
            # å¤„ç†æ—¥æœŸåˆ—
            date_column = "Cohort Day"
            if date_column not in df.columns:
                possible_date_columns = ['Date', 'date', 'æ—¥æœŸ', 'DAY', 'Day', 'day']
                for col in possible_date_columns:
                    if col in df.columns:
                        date_column = col
                        break
                
                if date_column not in df.columns:
                    log_container.error(f"æ— æ³•æ‰¾åˆ°æ—¥æœŸåˆ—")
                    continue
            
            # æ’åºæ•°æ®
            try:
                df[date_column] = pd.to_datetime(df[date_column])
                df = df.sort_values(by=date_column)
            except:
                try:
                    df = df.sort_values(by=date_column)
                except:
                    log_container.warning(f"æ— æ³•æ’åºæ•°æ®")
            
            # æ£€æŸ¥ç”¨æˆ·åˆ—
            users_column = 'Users'
            if users_column not in df.columns:
                possible_users_columns = ['users', 'ç”¨æˆ·æ•°', 'DAU', 'User Count', 'user_count']
                for col in possible_users_columns:
                    if col in df.columns:
                        users_column = col
                        break
                
                if users_column not in df.columns:
                    log_container.error(f"æ— æ³•æ‰¾åˆ°ç”¨æˆ·åˆ—")
                    continue
            
            # æ·»åŠ ç©ºåˆ—
            config = channels_config[channel]
            for j in range(config['empty_columns']):
                df[' ' * (j + 1)] = None
            
            # è®¡ç®—ç•™å­˜ç‡
            for day in config['days']:
                retention_column = f'sessions - Unique users - day {day} - partial'
                alternative_column = f'sessions - Unique users - day {day}'
                
                if retention_column in df.columns:
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
                elif alternative_column in df.columns:
                    df[retention_column] = df[alternative_column]
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
                else:
                    log_container.warning(f"æ— æ³•è®¡ç®— day{day} ç•™å­˜ç‡")
            
            processed_data[channel] = df
            log_container.success(f"æˆåŠŸå¤„ç† {channel} æ¸ é“ç•™å­˜æ•°æ®")
            
        except Exception as e:
            log_container.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"ç•™å­˜æ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {len(processed_data)} ä¸ªæ–‡ä»¶")
    
    return processed_data if processed_data else None

def create_integrated_dau(merged_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆä¸‰ä¸ªæ¸ é“çš„DAUæ•°æ®"""
    if not merged_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    for channel, df in merged_data.items():
        df_copy = df.copy()
        # æ˜ å°„æ¸ é“å
        channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios'}
        df_copy.insert(1, 'ä¸‰ç«¯', channel_mapping.get(channel, channel))
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # ç»Ÿä¸€åˆ—
        all_columns = []
        for df in integrated_dfs:
            all_columns.extend(df.columns.tolist())
        
        unique_columns = list(dict.fromkeys(all_columns))
        
        standardized_dfs = []
        for df in integrated_dfs:
            df_copy = df.copy()
            
            for col in unique_columns:
                if col not in df_copy.columns:
                    df_copy[col] = 'N/A'
            
            df_copy = df_copy[unique_columns]
            standardized_dfs.append(df_copy)
        
        integrated_df = pd.concat(standardized_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if 'date' in integrated_df.columns:
            integrated_df['date'] = integrated_df['date'].apply(force_standardize_date)
        
        # æ’åº
        try:
            integrated_df['sort_key'] = integrated_df['date'].apply(convert_date_to_sortable)
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=['sort_key', 'æ¸ é“æ’åº'])
            integrated_df = integrated_df.drop(columns=['sort_key', 'æ¸ é“æ’åº'])
        except Exception as e:
            st.warning(f"æ•´åˆDAUæ•°æ®æ’åºæ—¶å‡ºé”™: {str(e)}")
        
        integrated_df = integrated_df.fillna('N/A')
        
        # åªä¿ç•™å‰15åˆ—åŠ ä¸‰ç«¯åˆ—
        if len(integrated_df.columns) > 16:
            first_15_cols = integrated_df.iloc[:, :15].columns.tolist()
            if "ä¸‰ç«¯" in integrated_df.columns and "ä¸‰ç«¯" not in first_15_cols:
                cols_to_keep = first_15_cols + ["ä¸‰ç«¯"]
                integrated_df = integrated_df[cols_to_keep]
        
        return integrated_df
        
    except Exception as e:
        st.error(f"æ•´åˆDAUæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def create_integrated_retention(retention_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆç•™å­˜æ•°æ®"""
    if not retention_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    # æ¸ é“æ˜ å°„
    channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios', 'ios_formal': 'ios'}
    
    # æœŸæœ›çš„åˆ—é¡ºåº
    expected_columns = [
        "Cohort Day", "Ltv Country", "Campaign Id", "Keywords", "Users", "Cost", "Average eCPI",
        "sessions - Unique users - day 1 - partial", "sessions - Unique users - day 2 - partial",
        "sessions - Unique users - day 3 - partial", "sessions - Unique users - day 4 - partial",
        "sessions - Unique users - day 5 - partial", "sessions - Unique users - day 6 - partial",
        "sessions - Unique users - day 7 - partial", "sessions - Unique users - day 14 - partial",
        "sessions - Unique users - day 30 - partial", "Unnamed: 16", "Unnamed: 17", "Unnamed: 18",
        "day1", "day2", "day3", "day4", "day5", "day6", "day7", "day14", "day30",
        "ä¸‰ç«¯"
    ]
    
    for channel, df in retention_data.items():
        df_copy = df.copy()
        
        # æ˜ å°„æ¸ é“å
        mapped_channel = channel_mapping.get(channel, channel)
        df_copy["ä¸‰ç«¯"] = mapped_channel
        
        # åˆ—åä¿®æ­£
        if channel in ["mvp", "and"]:
            alt_day14 = "sessions - Unique users - day 14- partial"
            std_day14 = "sessions - Unique users - day 14 - partial"
            if alt_day14 in df_copy.columns and std_day14 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day14: std_day14})
            
            alt_day30 = "sessions - Unique users - day 30- partial"
            std_day30 = "sessions - Unique users - day 30 - partial"
            if alt_day30 in df_copy.columns and std_day30 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day30: std_day30})
        
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # åˆå¹¶æ•°æ®
        integrated_df = pd.concat(integrated_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if "Cohort Day" in integrated_df.columns:
            integrated_df["Cohort Day"] = integrated_df["Cohort Day"].apply(force_standardize_date)
        
        # é‡æ–°æ’åºåˆ—å¹¶å¡«å……ç¼ºå¤±åˆ—
        final_columns = []
        for col in expected_columns:
            if col not in integrated_df.columns:
                integrated_df[col] = None
            final_columns.append(col)
        
        existing_cols = [col for col in final_columns if col in integrated_df.columns]
        integrated_df = integrated_df[existing_cols]
        
        # æ¸…ç©ºUnnamedåˆ—
        unnamed_cols = [col for col in integrated_df.columns if 'Unnamed' in col]
        for col in unnamed_cols:
            integrated_df[col] = ''
        
        # æ’åº
        try:
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=["Cohort Day", "æ¸ é“æ’åº"])
            integrated_df = integrated_df.drop(columns=["æ¸ é“æ’åº"])
        except Exception as e:
            st.warning(f"æ•´åˆç•™å­˜æ•°æ®æ’åºæ—¶å‡ºé”™: {str(e)}")
        
        integrated_df = integrated_df.fillna('N/A')
        
        return integrated_df
        
    except Exception as e:
        st.error(f"æ•´åˆç•™å­˜æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def main():
    st.set_page_config(
        page_title="å®Œæ•´æ•°æ®å¤„ç†å·¥å…·",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š å®Œæ•´æ•°æ®å¤„ç†å·¥å…·")
    st.markdown("**DAUåˆå¹¶ + ç•™å­˜ç‡è®¡ç®— + æ•°æ®æ•´åˆ**")
    st.markdown("---")
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“‹ ä½¿ç”¨è¯´æ˜", expanded=True):
        st.markdown("""
        ### ğŸ¯ åŠŸèƒ½æ¦‚è¿°
        - **DAUæ–‡ä»¶åˆå¹¶**: å¤„ç†å¤šä¸ªDAU CSVæ–‡ä»¶ï¼ŒæŒ‰æ¸ é“åˆ†ç»„åˆå¹¶
        - **ç•™å­˜ç‡è®¡ç®—**: å¤„ç†ç•™å­˜æ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨è®¡ç®—å„å¤©ç•™å­˜ç‡
        - **æ•°æ®æ•´åˆ**: ç”Ÿæˆå®Œæ•´çš„ä¸‰ç«¯æ•°æ®æ–‡ä»¶å’Œåˆ†æ¸ é“æ–‡ä»¶
        
        ### ğŸ“ æ–‡ä»¶è¦æ±‚
        **DAUæ–‡ä»¶å‘½å**: `dau_æ¸ é“_æ—¥æœŸ.csv` (ä¾‹å¦‚: `dau_mvp_3.17.csv`)
        - æ”¯æŒæ¸ é“: mvp, and, ios
        
        **ç•™å­˜æ–‡ä»¶å‘½å**: 
        - `retention_ios.csv` (iOSæ¸ é“)
        - `retention_ios_formal.csv` (iOSæ­£å¼æ¸ é“)
        - `retention_mvp.csv` (MVPæ¸ é“)
        - `retention_and.csv` (Androidæ¸ é“)
        
        ### ğŸ“¤ è¾“å‡ºæ–‡ä»¶
        - **ä¸‰ç«¯DAUæ±‡æ€»æ–‡ä»¶**: åŒ…å«æ‰€æœ‰æ¸ é“DAUæ•°æ®
        - **ä¸‰ç«¯ç•™å­˜æ±‡æ€»æ–‡ä»¶**: åŒ…å«æ‰€æœ‰æ¸ é“ç•™å­˜æ•°æ®
        - **å„æ¸ é“å•ç‹¬æ–‡ä»¶**: DAUå’Œç•™å­˜çš„åˆ†æ¸ é“æ–‡ä»¶
        """)
    
    # åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2 = st.tabs(["ğŸ“ˆ DAUæ–‡ä»¶å¤„ç†", "ğŸ”„ ç•™å­˜æ–‡ä»¶å¤„ç†"])
    
    # å­˜å‚¨å¤„ç†ç»“æœ
    if 'dau_results' not in st.session_state:
        st.session_state.dau_results = None
    if 'retention_results' not in st.session_state:
        st.session_state.retention_results = None
    
    # DAUæ–‡ä»¶å¤„ç†æ ‡ç­¾é¡µ
    with tab1:
        st.subheader("ğŸ“ ä¸Šä¼ DAUæ–‡ä»¶")
        dau_files = st.file_uploader(
            "é€‰æ‹©DAU CSVæ–‡ä»¶",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶åæ ¼å¼: dau_æ¸ é“_æ—¥æœŸ.csv",
            key="dau_uploader"
        )
        
        if dau_files:
            st.success(f"å·²é€‰æ‹© {len(dau_files)} ä¸ªDAUæ–‡ä»¶")
            
            if st.button("ğŸš€ å¤„ç†DAUæ–‡ä»¶", type="primary", key="process_dau"):
                with st.spinner("æ­£åœ¨å¤„ç†DAUæ–‡ä»¶..."):
                    st.session_state.dau_results = process_dau_files(dau_files)
                
                if st.session_state.dau_results:
                    st.success("âœ… DAUæ–‡ä»¶å¤„ç†å®Œæˆ!")
    
    # ç•™å­˜æ–‡ä»¶å¤„ç†æ ‡ç­¾é¡µ
    with tab2:
        st.subheader("ğŸ“ ä¸Šä¼ ç•™å­˜æ–‡ä»¶")
        retention_files = st.file_uploader(
            "é€‰æ‹©ç•™å­˜CSVæ–‡ä»¶",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶åæ ¼å¼: retention_æ¸ é“.csv",
            key="retention_uploader"
        )
        
        if retention_files:
            st.success(f"å·²é€‰æ‹© {len(retention_files)} ä¸ªç•™å­˜æ–‡ä»¶")
            
            if st.button("ğŸš€ å¤„ç†ç•™å­˜æ–‡ä»¶", type="primary", key="process_retention"):
                with st.spinner("æ­£åœ¨å¤„ç†ç•™å­˜æ–‡ä»¶..."):
                    st.session_state.retention_results = process_retention_files(retention_files)
                
                if st.session_state.retention_results:
                    st.success("âœ… ç•™å­˜æ–‡ä»¶å¤„ç†å®Œæˆ!")
    
    # å¦‚æœæœ‰å¤„ç†ç»“æœï¼Œæ˜¾ç¤ºæ•°æ®é¢„è§ˆå’Œä¸‹è½½é€‰é¡¹
    if st.session_state.dau_results or st.session_state.retention_results:
        st.markdown("---")
        st.subheader("ğŸ“Š å¤„ç†ç»“æœ")
        
        # åˆ›å»ºç»“æœæ ‡ç­¾é¡µ
        result_tabs = []
        if st.session_state.dau_results:
            result_tabs.append("ğŸ“ˆ DAUæ•°æ®")
        if st.session_state.retention_results:
            result_tabs.append("ğŸ”„ ç•™å­˜æ•°æ®")
        
        if result_tabs:
            tabs = st.tabs(result_tabs)
            tab_index = 0
            
            # DAUç»“æœæ˜¾ç¤º
            if st.session_state.dau_results:
                with tabs[tab_index]:
                    dau_data = st.session_state.dau_results
                    
                    # åˆ›å»ºæ•´åˆçš„DAUæ•°æ®
                    integrated_dau = create_integrated_dau(dau_data)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¤„ç†æ¸ é“æ•°", len(dau_data))
                    with col2:
                        total_rows = sum(len(df) for df in dau_data.values())
                        st.metric("æ€»æ•°æ®è¡Œæ•°", total_rows)
                    with col3:
                        if not integrated_dau.empty:
                            st.metric("æ•´åˆåè¡Œæ•°", len(integrated_dau))
                    
                    # æ•°æ®é¢„è§ˆ
                    preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯DAUæ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in dau_data.keys()])
                    
                    # ä¸‰ç«¯æ±‡æ€»é¢„è§ˆ
                    with preview_tabs[0]:
                        if not integrated_dau.empty:
                            st.dataframe(integrated_dau.head(10), use_container_width=True)
                        else:
                            st.error("æ— æ³•åˆ›å»ºä¸‰ç«¯DAUæ±‡æ€»æ•°æ®")
                    
                    # å„æ¸ é“é¢„è§ˆ
                    for i, (channel, df) in enumerate(dau_data.items()):
                        with preview_tabs[i + 1]:
                            st.dataframe(df.head(10), use_container_width=True)
                
                tab_index += 1
            
            # ç•™å­˜ç»“æœæ˜¾ç¤º
            if st.session_state.retention_results:
                with tabs[tab_index]:
                    retention_data = st.session_state.retention_results
                    
                    # åˆ›å»ºæ•´åˆçš„ç•™å­˜æ•°æ®
                    integrated_retention = create_integrated_retention(retention_data)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¤„ç†æ¸ é“æ•°", len(retention_data))
                    with col2:
                        total_rows = sum(len(df) for df in retention_data.values())
                        st.metric("æ€»æ•°æ®è¡Œæ•°", total_rows)
                    with col3:
                        if not integrated_retention.empty:
                            st.metric("æ•´åˆåè¡Œæ•°", len(integrated_retention))
                    
                    # æ•°æ®é¢„è§ˆ
                    preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯ç•™å­˜æ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in retention_data.keys()])
                    
                    # ä¸‰ç«¯æ±‡æ€»é¢„è§ˆ
                    with preview_tabs[0]:
                        if not integrated_retention.empty:
                            st.dataframe(integrated_retention.head(10), use_container_width=True)
                        else:
                            st.error("æ— æ³•åˆ›å»ºä¸‰ç«¯ç•™å­˜æ±‡æ€»æ•°æ®")
                    
                    # å„æ¸ é“é¢„è§ˆ
                    for i, (channel, df) in enumerate(retention_data.items()):
                        with preview_tabs[i + 1]:
                            st.dataframe(df.head(10), use_container_width=True)
        
        # ä¸‹è½½åŒºåŸŸ
        st.markdown("---")
        st.subheader("ğŸ’¾ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶")
        
        today = datetime.datetime.now().strftime("%m.%d")
        
        # ä¸»è¦ä¸‹è½½é€‰é¡¹
        st.markdown("### ğŸ¯ **æ±‡æ€»æ–‡ä»¶ä¸‹è½½**")
        
        download_cols = st.columns(2)
        
        # DAUæ±‡æ€»ä¸‹è½½
        if st.session_state.dau_results:
            with download_cols[0]:
                integrated_dau = create_integrated_dau(st.session_state.dau_results)
                if not integrated_dau.empty:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = integrated_dau.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“ˆ ä¸‹è½½ä¸‰ç«¯DAUæ±‡æ€»æ–‡ä»¶",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ä¸‰ç«¯dauæ±‡æ€».csv",
                        mime="text/csv",
                        type="primary"
                    )
                    st.success(f"âœ… {len(integrated_dau)} è¡ŒDAUæ•°æ®")
                else:
                    st.error("âŒ DAUæ±‡æ€»æ•°æ®ç”Ÿæˆå¤±è´¥")
        
        # ç•™å­˜æ±‡æ€»ä¸‹è½½
        if st.session_state.retention_results:
            with download_cols[1]:
                integrated_retention = create_integrated_retention(st.session_state.retention_results)
                if not integrated_retention.empty:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = integrated_retention.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ”„ ä¸‹è½½ä¸‰ç«¯ç•™å­˜æ±‡æ€»æ–‡ä»¶",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ä¸‰ç«¯ç•™å­˜æ±‡æ€».csv",
                        mime="text/csv",
                        type="primary"
                    )
                    st.success(f"âœ… {len(integrated_retention)} è¡Œç•™å­˜æ•°æ®")
                else:
                    st.error("âŒ ç•™å­˜æ±‡æ€»æ•°æ®ç”Ÿæˆå¤±è´¥")
        
        # åˆ†æ¸ é“æ–‡ä»¶ä¸‹è½½
        st.markdown("### ğŸ“ **åˆ†æ¸ é“æ–‡ä»¶ä¸‹è½½**")
        
        # DAUåˆ†æ¸ é“ä¸‹è½½
        if st.session_state.dau_results:
            st.markdown("**DAUåˆ†æ¸ é“æ–‡ä»¶:**")
            dau_cols = st.columns(len(st.session_state.dau_results))
            for i, (channel, df) in enumerate(st.session_state.dau_results.items()):
                with dau_cols[i]:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ“ˆ DAU-{channel.upper()}",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} dauæ±‡æ€»_{channel}.csv",
                        mime="text/csv",
                        key=f"dau_{channel}"
                    )
                    st.text(f"{len(df)} è¡Œæ•°æ®")
        
        # ç•™å­˜åˆ†æ¸ é“ä¸‹è½½
        if st.session_state.retention_results:
            st.markdown("**ç•™å­˜åˆ†æ¸ é“æ–‡ä»¶:**")
            retention_cols = st.columns(len(st.session_state.retention_results))
            for i, (channel, df) in enumerate(st.session_state.retention_results.items()):
                with retention_cols[i]:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ”„ ç•™å­˜-{channel.upper()}",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ç•™å­˜_{channel}.csv",
                        mime="text/csv",
                        key=f"retention_{channel}"
                    )
                    st.text(f"{len(df)} è¡Œæ•°æ®")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ ç›¸åº”çš„æ–‡ä»¶å¼€å§‹å¤„ç†")
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            <p>å®Œæ•´æ•°æ®å¤„ç†å·¥å…· | DAUåˆå¹¶ + ç•™å­˜è®¡ç®— + æ•°æ®æ•´åˆ</p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
