"""
å®Œæ•´æ•°æ®å¤„ç†å·¥å…· - é›†æˆç‰ˆï¼ˆä¼˜åŒ–å¸ƒå±€ï¼‰
========================

ç¯å¢ƒè¦æ±‚:
- Python 3.8+
- streamlit >= 1.28.0
- pandas >= 1.5.0
- numpy >= 1.20.0
- openpyxl >= 3.0.0

å®‰è£…å‘½ä»¤:
pip install streamlit pandas numpy openpyxl

è¿è¡Œå‘½ä»¤:
streamlit run app.py
"""

import streamlit as st
import pandas as pd
import datetime
import re
import io
import numpy as np
import warnings
from typing import Dict, List, Optional, Tuple

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

def convert_date_to_sortable(date_str: str) -> str:
    """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ’åºçš„æ ¼å¼"""
    try:
        parts = date_str.split('/')
        if len(parts) == 3:
            year, month, day = parts
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{year}{month}{day}"
    except:
        pass
    return date_str

def force_standardize_date(date_str):
    """å¼ºåˆ¶æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œç¡®ä¿æœˆä»½å’Œæ—¥æœŸéƒ½æ˜¯ä¸¤ä½æ•°"""
    if pd.isna(date_str) or str(date_str).strip() == '':
        return date_str

    date_str = str(date_str).strip()

    # å¤„ç†ä¸åŒåˆ†éš”ç¬¦ï¼Œç»Ÿä¸€è½¬æ¢ä¸º/
    if '-' in date_str:
        date_str = date_str.replace('-', '/')

    # å¦‚æœåŒ…å«/ï¼Œåˆ™å¤„ç†
    if '/' in date_str:
        parts = date_str.split('/')
        if len(parts) == 3:
            try:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                # å¼ºåˆ¶æ ¼å¼åŒ–ä¸ºä¸¤ä½æ•°
                result = f"{year:04d}/{month:02d}/{day:02d}"
                return result
            except:
                pass

    return date_str

def process_dau_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†DAUæ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    channel_dfs = {'mvp': [], 'and': [], 'ios': []}
    standard_columns = {'mvp': None, 'and': None, 'ios': None}
    
    processed_files = 0
    total_files = len(uploaded_files)
    
    log_container = st.expander("DAUæ–‡ä»¶å¤„ç†è¯¦æƒ…", expanded=False)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name
            
            if "dau" not in filename.lower():
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åä¸åŒ…å«'dau'")
                continue
            
            # ä»æ–‡ä»¶åä¸­æå–æ¸ é“ä¿¡æ¯
            if len(filename) > 7 and filename.startswith("dau_"):
                channel = filename[4:7]  # è·å–æ¸ é“å (mvp, and, ios)
                if channel not in channel_dfs:
                    log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ— æ³•è¯†åˆ«çš„æ¸ é“ '{channel}'")
                    continue
            else:
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                continue
            
            # è¯»å–CSVæ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                try:
                    df = pd.read_csv(io.StringIO(content.decode('utf-8')), 
                                   na_values=[''], keep_default_na=False)
                except UnicodeDecodeError:
                    df = pd.read_csv(io.StringIO(content.decode('latin1')), 
                                   na_values=[''], keep_default_na=False)
                
                log_container.success(f"æˆåŠŸè¯»å– {filename}, å½¢çŠ¶: {df.shape}")
                
            except Exception as e:
                log_container.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
                continue
            
            if df.empty:
                log_container.warning(f"æ–‡ä»¶ {filename} ä¸åŒ…å«æ•°æ®ï¼Œå·²è·³è¿‡")
                continue
            
            # åˆ é™¤æŒ‡å®šçš„ä¸‰åˆ—
            columns_to_drop = ['Total Conversions', 'Re-attribution', 'Re-engagement']
            original_cols = df.columns.tolist()
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')
            removed_cols = [col for col in columns_to_drop if col in original_cols]
            if removed_cols:
                log_container.info(f"å·²åˆ é™¤åˆ—: {', '.join(removed_cols)}")
            
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            try:
                date_part = filename.split('_')[-1].replace('.csv', '')
                match = re.search(r'(\d+)\.(\d+)', date_part)
                if match:
                    month, day = match.groups()
                    formatted_date = f"2025/{month}/{day}"
                else:
                    formatted_date = date_part
            except:
                formatted_date = "2025/1/1"
                log_container.warning(f"æ— æ³•ä»æ–‡ä»¶å {filename} æå–æ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            # æ·»åŠ æ—¥æœŸåˆ—
            df.insert(0, 'date', formatted_date)
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios' and 'Average eCPIUS$2.50' in df.columns:
                df = df.drop(columns=['Average eCPIUS$2.50'])
                log_container.info(f"ç§»é™¤iOSä¸­çš„é—®é¢˜åˆ—")
            
            # åˆ—æ ‡å‡†åŒ–
            if standard_columns[channel] is None:
                standard_columns[channel] = df.columns.tolist()
            else:
                current_cols = df.columns.tolist()
                if current_cols != standard_columns[channel]:
                    missing_cols = [col for col in standard_columns[channel] if col not in current_cols]
                    extra_cols = [col for col in current_cols if col not in standard_columns[channel]]
                    
                    if missing_cols:
                        for col in missing_cols:
                            df[col] = 'N/A'
                    
                    if extra_cols:
                        df = df.drop(columns=extra_cols)
                    
                    df = df[standard_columns[channel]]
            
            df = df.fillna('N/A')
            channel_dfs[channel].append(df)
            processed_files += 1
            
        except Exception as e:
            log_container.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"DAUæ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶")
    
    if processed_files == 0:
        st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•DAUæ–‡ä»¶")
        return None
    
    # åˆå¹¶æ•°æ®
    merged_by_channel = {}
    
    for channel, df_list in channel_dfs.items():
        if df_list:
            # ç¡®ä¿åˆ—ä¸€è‡´æ€§
            if len(df_list) > 1:
                standard_cols = list(df_list[0].columns)
                for i, df in enumerate(df_list):
                    if set(df.columns) != set(standard_cols):
                        missing = [col for col in standard_cols if col not in df.columns]
                        extra = [col for col in df.columns if col not in standard_cols]
                        
                        for col in missing:
                            df[col] = 'N/A'
                        if extra:
                            df = df.drop(columns=extra)
                        df = df[standard_cols]
                        df_list[i] = df
            
            merged_df = pd.concat(df_list, ignore_index=True)
            
            # æŒ‰æ—¥æœŸæ’åº
            try:
                merged_df['sort_key'] = merged_df['date'].apply(convert_date_to_sortable)
                merged_df = merged_df.sort_values(by='sort_key')
                merged_df = merged_df.drop(columns=['sort_key'])
            except Exception as e:
                st.warning(f"æ¸ é“ {channel} æ’åºæ—¶å‡ºé”™: {str(e)}")
            
            merged_df = merged_df.fillna('N/A')
            
            # iOSç‰¹æ®Šå¤„ç†
            if channel == 'ios':
                expected_columns = ['date', 'Country', 'Impressions', 'Clicks', 'Installs', 'Conversion Rate',
                                  'Activity Sessions', 'Cost', 'Activity Revenue', 'Average eCPIUS$2.31',
                                  'Average DAU', 'Average MAU', 'Average DAU/MAU Rate', 'ARPDAU']
                
                extra_columns = [col for col in merged_df.columns if col not in expected_columns]
                if extra_columns:
                    merged_df = merged_df.drop(columns=extra_columns)
                
                missing_columns = [col for col in expected_columns if col not in merged_df.columns]
                if missing_columns:
                    for col in missing_columns:
                        merged_df[col] = 'N/A'
                
                merged_df = merged_df[expected_columns]
            
            merged_by_channel[channel] = merged_df
    
    return merged_by_channel

def process_retention_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†ç•™å­˜æ–‡ä»¶ä¸Šä¼ """
    if not uploaded_files:
        return None
        
    # æ¸ é“é…ç½®
    channels_config = {
        'ios': {
            'pattern': 'retention_ios.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'ios_formal': {
            'pattern': 'retention_ios_formal.csv',
            'empty_columns': 4,
            'days': list(range(1, 8)) + [14, 30]
        },
        'mvp': {
            'pattern': 'retention_mvp.csv',
            'empty_columns': 1,
            'days': list(range(1, 8)) + [14]
        },
        'and': {
            'pattern': 'retention_and.csv',
            'empty_columns': 0,
            'days': list(range(1, 8)) + [14, 30]
        }
    }
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.expander("ç•™å­˜æ–‡ä»¶å¤„ç†è¯¦æƒ…", expanded=False)
    
    processed_data = {}
    total_files = len(uploaded_files)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name.lower()
            
            # åŒ¹é…æ¸ é“
            channel = None
            for ch, config in channels_config.items():
                if config['pattern'].lower() in filename:
                    channel = ch
                    break
            
            if not channel:
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {uploaded_file.name}: æ— æ³•è¯†åˆ«çš„ç•™å­˜æ–‡ä»¶")
                continue
            
            # è¯»å–æ–‡ä»¶
            try:
                content = uploaded_file.getvalue()
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(io.StringIO(content.decode(encoding)))
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df is None:
                    log_container.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {uploaded_file.name}")
                    continue
                
                log_container.success(f"æˆåŠŸè¯»å– {uploaded_file.name}, å½¢çŠ¶: {df.shape}")
                
            except Exception as e:
                log_container.error(f"è¯»å–æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
                continue
            
            # å¤„ç†æ—¥æœŸåˆ—
            date_column = "Cohort Day"
            if date_column not in df.columns:
                possible_date_columns = ['Date', 'date', 'æ—¥æœŸ', 'DAY', 'Day', 'day']
                for col in possible_date_columns:
                    if col in df.columns:
                        date_column = col
                        break
                
                if date_column not in df.columns:
                    log_container.error(f"æ— æ³•æ‰¾åˆ°æ—¥æœŸåˆ—")
                    continue
            
            # æ’åºæ•°æ®
            try:
                df[date_column] = pd.to_datetime(df[date_column])
                df = df.sort_values(by=date_column)
            except:
                try:
                    df = df.sort_values(by=date_column)
                except:
                    log_container.warning(f"æ— æ³•æ’åºæ•°æ®")
            
            # æ£€æŸ¥ç”¨æˆ·åˆ—
            users_column = 'Users'
            if users_column not in df.columns:
                possible_users_columns = ['users', 'ç”¨æˆ·æ•°', 'DAU', 'User Count', 'user_count']
                for col in possible_users_columns:
                    if col in df.columns:
                        users_column = col
                        break
                
                if users_column not in df.columns:
                    log_container.error(f"æ— æ³•æ‰¾åˆ°ç”¨æˆ·åˆ—")
                    continue
            
            # æ·»åŠ ç©ºåˆ—
            config = channels_config[channel]
            for j in range(config['empty_columns']):
                df[' ' * (j + 1)] = None
            
            # è®¡ç®—ç•™å­˜ç‡
            for day in config['days']:
                retention_column = f'sessions - Unique users - day {day} - partial'
                alternative_column = f'sessions - Unique users - day {day}'
                
                if retention_column in df.columns:
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
                elif alternative_column in df.columns:
                    df[retention_column] = df[alternative_column]
                    df[f'day{day}'] = (df[retention_column] / df[users_column]).round(4)
                else:
                    log_container.warning(f"æ— æ³•è®¡ç®— day{day} ç•™å­˜ç‡")
            
            processed_data[channel] = df
            log_container.success(f"æˆåŠŸå¤„ç† {channel} æ¸ é“ç•™å­˜æ•°æ®")
            
        except Exception as e:
            log_container.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    progress_bar.progress(1.0)
    status_text.text(f"ç•™å­˜æ–‡ä»¶å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {len(processed_data)} ä¸ªæ–‡ä»¶")
    
    return processed_data if processed_data else None

def create_integrated_dau(merged_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆä¸‰ä¸ªæ¸ é“çš„DAUæ•°æ®"""
    if not merged_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    for channel, df in merged_data.items():
        df_copy = df.copy()
        # æ˜ å°„æ¸ é“å
        channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios'}
        df_copy.insert(1, 'ä¸‰ç«¯', channel_mapping.get(channel, channel))
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # ç»Ÿä¸€åˆ—
        all_columns = []
        for df in integrated_dfs:
            all_columns.extend(df.columns.tolist())
        
        unique_columns = list(dict.fromkeys(all_columns))
        
        standardized_dfs = []
        for df in integrated_dfs:
            df_copy = df.copy()
            
            for col in unique_columns:
                if col not in df_copy.columns:
                    df_copy[col] = 'N/A'
            
            df_copy = df_copy[unique_columns]
            standardized_dfs.append(df_copy)
        
        integrated_df = pd.concat(standardized_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if 'date' in integrated_df.columns:
            integrated_df['date'] = integrated_df['date'].apply(force_standardize_date)
        
        # æ’åº
        try:
            integrated_df['sort_key'] = integrated_df['date'].apply(convert_date_to_sortable)
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=['sort_key', 'æ¸ é“æ’åº'])
            integrated_df = integrated_df.drop(columns=['sort_key', 'æ¸ é“æ’åº'])
        except Exception as e:
            st.warning(f"æ•´åˆDAUæ•°æ®æ’åºæ—¶å‡ºé”™: {str(e)}")
        
        integrated_df = integrated_df.fillna('N/A')
        
        # åªä¿ç•™å‰15åˆ—åŠ ä¸‰ç«¯åˆ—
        if len(integrated_df.columns) > 16:
            first_15_cols = integrated_df.iloc[:, :15].columns.tolist()
            if "ä¸‰ç«¯" in integrated_df.columns and "ä¸‰ç«¯" not in first_15_cols:
                cols_to_keep = first_15_cols + ["ä¸‰ç«¯"]
                integrated_df = integrated_df[cols_to_keep]
        
        return integrated_df
        
    except Exception as e:
        st.error(f"æ•´åˆDAUæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def create_integrated_retention(retention_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """æ•´åˆç•™å­˜æ•°æ®"""
    if not retention_data:
        return pd.DataFrame()
    
    integrated_dfs = []
    
    # æ¸ é“æ˜ å°„
    channel_mapping = {'and': 'android', 'mvp': 'mvp', 'ios': 'ios', 'ios_formal': 'ios'}
    
    # æœŸæœ›çš„åˆ—é¡ºåº
    expected_columns = [
        "Cohort Day", "Ltv Country", "Campaign Id", "Keywords", "Users", "Cost", "Average eCPI",
        "sessions - Unique users - day 1 - partial", "sessions - Unique users - day 2 - partial",
        "sessions - Unique users - day 3 - partial", "sessions - Unique users - day 4 - partial",
        "sessions - Unique users - day 5 - partial", "sessions - Unique users - day 6 - partial",
        "sessions - Unique users - day 7 - partial", "sessions - Unique users - day 14 - partial",
        "sessions - Unique users - day 30 - partial", "Unnamed: 16", "Unnamed: 17", "Unnamed: 18",
        "day1", "day2", "day3", "day4", "day5", "day6", "day7", "day14", "day30",
        "ä¸‰ç«¯"
    ]
    
    for channel, df in retention_data.items():
        df_copy = df.copy()
        
        # æ˜ å°„æ¸ é“å
        mapped_channel = channel_mapping.get(channel, channel)
        df_copy["ä¸‰ç«¯"] = mapped_channel
        
        # åˆ—åä¿®æ­£
        if channel in ["mvp", "and"]:
            alt_day14 = "sessions - Unique users - day 14- partial"
            std_day14 = "sessions - Unique users - day 14 - partial"
            if alt_day14 in df_copy.columns and std_day14 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day14: std_day14})
            
            alt_day30 = "sessions - Unique users - day 30- partial"
            std_day30 = "sessions - Unique users - day 30 - partial"
            if alt_day30 in df_copy.columns and std_day30 not in df_copy.columns:
                df_copy = df_copy.rename(columns={alt_day30: std_day30})
        
        integrated_dfs.append(df_copy)
    
    if not integrated_dfs:
        return pd.DataFrame()
    
    try:
        # åˆå¹¶æ•°æ®
        integrated_df = pd.concat(integrated_dfs, ignore_index=True)
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
        if "Cohort Day" in integrated_df.columns:
            integrated_df["Cohort Day"] = integrated_df["Cohort Day"].apply(force_standardize_date)
        
        # é‡æ–°æ’åºåˆ—å¹¶å¡«å……ç¼ºå¤±åˆ—
        final_columns = []
        for col in expected_columns:
            if col not in integrated_df.columns:
                integrated_df[col] = None
            final_columns.append(col)
        
        existing_cols = [col for col in final_columns if col in integrated_df.columns]
        integrated_df = integrated_df[existing_cols]
        
        # æ¸…ç©ºUnnamedåˆ—
        unnamed_cols = [col for col in integrated_df.columns if 'Unnamed' in col]
        for col in unnamed_cols:
            integrated_df[col] = ''
        
        # æ’åº
        try:
            channel_order = {"mvp": 0, "android": 1, "ios": 2}
            integrated_df["æ¸ é“æ’åº"] = integrated_df["ä¸‰ç«¯"].map(channel_order)
            integrated_df = integrated_df.sort_values(by=["Cohort Day", "æ¸ é“æ’åº"])
            integrated_df = integrated_df.drop(columns=["æ¸ é“æ’åº"])
        except Exception as e:
            st.warning(f"æ•´åˆç•™å­˜æ•°æ®æ’åºæ—¶å‡ºé”™: {str(e)}")
        
        integrated_df = integrated_df.fillna('N/A')
        
        return integrated_df
        
    except Exception as e:
        st.error(f"æ•´åˆç•™å­˜æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def delete_excel_by_date_interface():
    """åº•è¡¨æ—¥æœŸåˆ é™¤ç•Œé¢"""
    st.subheader("ğŸ“… åº•è¡¨æ—¥æœŸåˆ é™¤åŠŸèƒ½")
    
    # ä¸Šä¼ Excelæ–‡ä»¶
    uploaded_excel = st.file_uploader(
        "ä¸Šä¼ åº•è¡¨Excelæ–‡ä»¶",
        type=['xlsx'],
        help="è¯·ä¸Šä¼ éœ€è¦åˆ é™¤æ—¥æœŸçš„Excelåº•è¡¨æ–‡ä»¶",
        key="excel_uploader"
    )
    
    if uploaded_excel:
        st.success(f"å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_excel.name}")
        
        # è®¡ç®—é»˜è®¤æ—¥æœŸï¼ˆä»Šå¤©-2å¤©ï¼‰
        from datetime import datetime, timedelta
        default_date = datetime.now() - timedelta(days=2)
        default_date_mmdd = default_date.strftime("%m%d")
        default_date_display = default_date.strftime("%Y/%m/%d")
        
        st.info(f"é»˜è®¤æˆªæ­¢æ—¥æœŸï¼ˆä»Šå¤©-2å¤©ï¼‰: {default_date_display}")
        
        # åˆ›å»ºä¸¤åˆ—è¾“å…¥
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ä¸‰ç«¯DAUæˆªæ­¢æ—¥æœŸ**")
            dau_date = st.text_input(
                "æ ¼å¼: MMDD (å¦‚: 0601)",
                value=default_date_mmdd,
                help="åˆ é™¤è¯¥æ—¥æœŸåŠä¹‹åçš„æ‰€æœ‰DAUæ•°æ®",
                key="dau_date"
            )
        
        with col2:
            st.markdown("**ä¸‰ç«¯ç•™å­˜æˆªæ­¢æ—¥æœŸ**")
            retention_date = st.text_input(
                "æ ¼å¼: MMDD (å¦‚: 0601)",
                value="",
                help="åˆ é™¤è¯¥æ—¥æœŸåŠä¹‹åçš„æ‰€æœ‰ç•™å­˜æ•°æ®",
                key="retention_date"
            )
        
        if st.button("ğŸ—‘ï¸ æ‰§è¡Œåˆ é™¤æ“ä½œ", type="primary"):
            if not retention_date:
                st.error("è¯·è¾“å…¥ä¸‰ç«¯ç•™å­˜æˆªæ­¢æ—¥æœŸ")
                return
            
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼
                for date_input, name in [(dau_date, "DAU"), (retention_date, "ç•™å­˜")]:
                    if len(date_input) != 4 or not date_input.isdigit():
                        st.error(f"{name}æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥4ä½æ•°å­—")
                        return
                
                # è½¬æ¢æ—¥æœŸæ ¼å¼
                dau_month, dau_day = dau_date[:2], dau_date[2:]
                ret_month, ret_day = retention_date[:2], retention_date[2:]
                
                dau_cutoff = f"2025/{dau_month}/{dau_day}"
                ret_cutoff = f"2025/{ret_month}/{ret_day}"
                
                # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                pd.to_datetime(dau_cutoff)
                pd.to_datetime(ret_cutoff)
                
                st.info(f"å°†åˆ é™¤:\n- ä¸‰ç«¯DAU: {dau_cutoff} åŠä¹‹åçš„æ•°æ®\n- ä¸‰ç«¯ç•™å­˜: {ret_cutoff} åŠä¹‹åçš„æ•°æ®")
                
                with st.spinner("æ­£åœ¨å¤„ç†Excelæ–‡ä»¶..."):
                    # è¯»å–Excelæ–‡ä»¶
                    excel_content = uploaded_excel.getvalue()
                    excel_file = pd.ExcelFile(io.BytesIO(excel_content))
                    
                    all_sheets_data = {}
                    target_sheets = ['ä¸‰ç«¯ç•™å­˜', 'ä¸‰ç«¯DAU']
                    
                    for sheet_name in excel_file.sheet_names:
                        try:
                            df = pd.read_excel(io.BytesIO(excel_content), sheet_name=sheet_name)
                            
                            if sheet_name not in target_sheets:
                                all_sheets_data[sheet_name] = df
                                continue
                            
                            # è·å–æˆªæ­¢æ—¥æœŸ
                            cutoff_date_str = dau_cutoff if sheet_name == 'ä¸‰ç«¯DAU' else ret_cutoff
                            
                            if len(df) == 0:
                                all_sheets_data[sheet_name] = df
                                continue
                            
                            # è·å–ç¬¬ä¸€åˆ—ä½œä¸ºæ—¥æœŸåˆ—
                            date_column = df.columns[0]
                            
                            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                            df_filtered = df.dropna(subset=[date_column]).copy()
                            
                            if len(df_filtered) == 0:
                                all_sheets_data[sheet_name] = pd.DataFrame()
                                continue
                            
                            # è½¬æ¢æ—¥æœŸ
                            df_filtered[date_column] = pd.to_datetime(df_filtered[date_column], errors='coerce')
                            df_filtered = df_filtered.dropna(subset=[date_column]).copy()
                            
                            if len(df_filtered) == 0:
                                all_sheets_data[sheet_name] = pd.DataFrame()
                                continue
                            
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                            df_filtered[date_column] = df_filtered[date_column].dt.strftime('%Y/%m/%d')
                            
                            # åˆ é™¤æŒ‡å®šæ—¥æœŸåŠä¹‹åçš„æ•°æ®
                            df_final = df_filtered[df_filtered[date_column] < cutoff_date_str].copy()
                            all_sheets_data[sheet_name] = df_final
                            
                        except Exception as e:
                            st.error(f"å¤„ç†sheet '{sheet_name}' æ—¶å‡ºé”™: {str(e)}")
                            all_sheets_data[sheet_name] = pd.DataFrame()
                    
                    # åˆ›å»ºæ–°çš„Excelæ–‡ä»¶
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        for sheet_name, data in all_sheets_data.items():
                            data.to_excel(writer, sheet_name=sheet_name, index=False)
                    
                    output.seek(0)
                    
                    st.success("âœ… åˆ é™¤æ“ä½œå®Œæˆ!")
                    
                    # æ˜¾ç¤ºåˆ é™¤ç»“æœç»Ÿè®¡
                    st.markdown("### åˆ é™¤ç»“æœç»Ÿè®¡")
                    for sheet_name in target_sheets:
                        if sheet_name in all_sheets_data:
                            data = all_sheets_data[sheet_name]
                            cutoff_used = dau_cutoff if sheet_name == 'ä¸‰ç«¯DAU' else ret_cutoff
                            if len(data) > 0:
                                date_col = data.columns[0]
                                min_date = data[date_col].min()
                                max_date = data[date_col].max()
                                st.write(f"**{sheet_name}**: {len(data)}è¡Œ (æˆªæ­¢: {cutoff_used}, æ—¥æœŸåŒºé—´: {min_date} è‡³ {max_date})")
                            else:
                                st.write(f"**{sheet_name}**: 0è¡Œ (æˆªæ­¢: {cutoff_used})")
                    
                    # æä¾›ä¸‹è½½
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„Excelæ–‡ä»¶",
                        data=output.getvalue(),
                        file_name=f"åº•è¡¨_åˆ é™¤å_{datetime.now().strftime('%m%d')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                
            except Exception as e:
                st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def validate_data_interface():
    """æ•°æ®æ ¡éªŒç•Œé¢"""
    st.subheader("ğŸ” æ•°æ®æ ¡éªŒåŠŸèƒ½")
    
    # ä¸Šä¼ æ–‡ä»¶
    col1, col2 = st.columns(2)
    
    with col1:
        excel_file = st.file_uploader(
            "ä¸Šä¼ åº•è¡¨Excelæ–‡ä»¶",
            type=['xlsx'],
            help="åŒ…å«ä¸‰ç«¯DAUå’Œä¸‰ç«¯ç•™å­˜æ•°æ®çš„Excelæ–‡ä»¶",
            key="validate_excel"
        )
    
    with col2:
        csv_file = st.file_uploader(
            "ä¸Šä¼ retention_all.csvæ–‡ä»¶",
            type=['csv'],
            help="ç”¨äºå¯¹æ¯”æ ¡éªŒçš„retention_all.csvæ–‡ä»¶",
            key="validate_csv"
        )
    
    if excel_file and csv_file:
        if st.button("ğŸš€ å¼€å§‹æ•°æ®æ ¡éªŒ", type="primary"):
            with st.spinner("æ­£åœ¨è¿›è¡Œæ•°æ®æ ¡éªŒ..."):
                try:
                    # è¯»å–Excelæ–‡ä»¶
                    excel_content = excel_file.getvalue()
                    
                    # è¯»å–ä¸‰ç«¯DAUæ•°æ®
                    dau_df = pd.read_excel(io.BytesIO(excel_content), sheet_name='ä¸‰ç«¯DAU')
                    
                    # è¯»å–ä¸‰ç«¯ç•™å­˜æ•°æ®
                    retention_df = pd.read_excel(io.BytesIO(excel_content), sheet_name='ä¸‰ç«¯ç•™å­˜')
                    
                    # è¯»å–CSVæ–‡ä»¶
                    csv_content = csv_file.getvalue()
                    retention_all_df = pd.read_csv(io.StringIO(csv_content.decode('utf-8')))
                    
                    # å¤„ç†retention_allæ•°æ®
                    def map_app_id_to_platform(app_id):
                        if app_id == 'com.weather.mjweather':
                            return 'android'
                        elif app_id == 'id6720731790':
                            return 'ios'
                        elif app_id == 'com.moji.international':
                            return 'mvp'
                        else:
                            return 'unknown'
                    
                    # å¯»æ‰¾App Idåˆ—
                    app_id_columns = ['App Id', 'app_id', 'AppId', 'app id']
                    app_id_col = None
                    for col in app_id_columns:
                        if col in retention_all_df.columns:
                            app_id_col = col
                            break
                    
                    if app_id_col:
                        retention_all_df['ä¸‰ç«¯'] = retention_all_df[app_id_col].apply(map_app_id_to_platform)
                    
                    # åˆ›å»ºæ•°æ®é€è§†è¡¨
                    st.markdown("### ğŸ“Š æ•°æ®é€è§†è¡¨åˆ†æ")
                    
                    # DAUé€è§†è¡¨
                    if not dau_df.empty:
                        date_col = dau_df.columns[0]
                        if 'ä¸‰ç«¯' in dau_df.columns and 'Installs' in dau_df.columns:
                            dau_df[date_col] = pd.to_datetime(dau_df[date_col])
                            dau_pivot = pd.pivot_table(
                                dau_df,
                                values='Installs',
                                index=date_col,
                                columns='ä¸‰ç«¯',
                                aggfunc='sum',
                                fill_value=0
                            ).sort_index(ascending=False).astype(int)
                            
                            st.markdown("**DAUæ•°æ®é€è§†è¡¨ (å‰10è¡Œ)**")
                            st.dataframe(dau_pivot.head(10))
                    
                    # ç•™å­˜é€è§†è¡¨
                    if not retention_df.empty:
                        date_col = retention_df.columns[0]
                        if 'ä¸‰ç«¯' in retention_df.columns and 'Users' in retention_df.columns:
                            retention_df[date_col] = pd.to_datetime(retention_df[date_col])
                            retention_pivot = pd.pivot_table(
                                retention_df,
                                values='Users',
                                index=date_col,
                                columns='ä¸‰ç«¯',
                                aggfunc='sum',
                                fill_value=0
                            ).sort_index(ascending=False).astype(int)
                            
                            st.markdown("**ç•™å­˜æ•°æ®é€è§†è¡¨ (å‰10è¡Œ)**")
                            st.dataframe(retention_pivot.head(10))
                    
                    # retention_allé€è§†è¡¨
                    if not retention_all_df.empty and app_id_col:
                        cohort_col = 'Cohort Day'
                        if cohort_col in retention_all_df.columns:
                            # å¯»æ‰¾æ•°å€¼åˆ—
                            exclude_cols = [cohort_col, app_id_col, 'ä¸‰ç«¯']
                            numeric_cols = [col for col in retention_all_df.columns 
                                          if col not in exclude_cols and pd.api.types.is_numeric_dtype(retention_all_df[col])]
                            
                            if numeric_cols:
                                value_col = numeric_cols[0]
                                retention_all_df[cohort_col] = pd.to_datetime(retention_all_df[cohort_col])
                                retention_all_pivot = pd.pivot_table(
                                    retention_all_df,
                                    values=value_col,
                                    index=cohort_col,
                                    columns='ä¸‰ç«¯',
                                    aggfunc='sum',
                                    fill_value=0
                                ).sort_index(ascending=False).astype(int)
                                
                                st.markdown("**Retention_allæ•°æ®é€è§†è¡¨ (å‰10è¡Œ)**")
                                st.dataframe(retention_all_pivot.head(10))
                                
                                # æ•°å€¼å¯¹æ¯”åˆ†æ
                                if 'retention_pivot' in locals():
                                    st.markdown("### ğŸ” æ•°å€¼å¯¹æ¯”åˆ†æ")
                                    
                                    overlapping_dates = set(retention_pivot.index).intersection(set(retention_all_pivot.index))
                                    
                                    if overlapping_dates:
                                        comparison_data = []
                                        
                                        for date in sorted(overlapping_dates, reverse=True)[:10]:  # æ˜¾ç¤ºæœ€è¿‘10å¤©
                                            retention_sum = retention_pivot.loc[date].sum()
                                            retention_all_sum = retention_all_pivot.loc[date].sum()
                                            
                                            if retention_all_sum > 0:
                                                difference = retention_sum - retention_all_sum
                                                percentage = (difference / retention_all_sum * 100) if retention_all_sum != 0 else 0
                                                
                                                comparison_data.append({
                                                    'æ—¥æœŸ': date.strftime('%Y-%m-%d'),
                                                    'ç•™å­˜æ€»å’Œ': retention_sum,
                                                    'Retention_allæ€»å’Œ': retention_all_sum,
                                                    'å·®å¼‚': difference,
                                                    'å·®å¼‚ç™¾åˆ†æ¯”': f"{percentage:.2f}%"
                                                })
                                        
                                        if comparison_data:
                                            comparison_df = pd.DataFrame(comparison_data)
                                            st.dataframe(comparison_df)
                                            
                                            # ç»Ÿè®¡æ‘˜è¦
                                            differences = [row['å·®å¼‚'] for row in comparison_data]
                                            if differences:
                                                st.markdown("**å¯¹æ¯”æ‘˜è¦:**")
                                                st.write(f"- å¹³å‡å·®å¼‚: {np.mean(differences):.0f}")
                                                st.write(f"- æœ€å¤§å·®å¼‚: {max(differences):.0f}")
                                                st.write(f"- æœ€å°å·®å¼‚: {min(differences):.0f}")
                    
                    st.success("âœ… æ•°æ®æ ¡éªŒå®Œæˆ!")
                    
                except Exception as e:
                    st.error(f"æ•°æ®æ ¡éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def main():
    st.set_page_config(
        page_title="æ—¥æŠ¥æ•°æ®å¤„ç†å·¥å…· - é›†æˆç‰ˆ",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š æ—¥æŠ¥æ•°æ®å¤„ç†å·¥å…· - é›†æˆç‰ˆ")
    st.markdown("**DAUåˆå¹¶ + ç•™å­˜ç‡è®¡ç®— + åº•è¡¨æ—¥æœŸåˆ é™¤ + æ®æ ¡éªŒ**")
    st.markdown("---")
    
    # ========== ä¸»è¦åŠŸèƒ½åŒºåŸŸ ==========
    # åˆ›å»ºå››ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ DAUæ–‡ä»¶å¤„ç†", "ğŸ”„ ç•™å­˜æ–‡ä»¶å¤„ç†", "ğŸ“… åº•è¡¨æ—¥æœŸåˆ é™¤", "ğŸ” æ•°æ®æ ¡éªŒ"])
    
    # å­˜å‚¨å¤„ç†ç»“æœ
    if 'dau_results' not in st.session_state:
        st.session_state.dau_results = None
    if 'retention_results' not in st.session_state:
        st.session_state.retention_results = None
    
    # DAUæ–‡ä»¶å¤„ç†æ ‡ç­¾é¡µ
    with tab1:
        st.subheader("ğŸ“ ä¸Šä¼ DAUæ–‡ä»¶")
        dau_files = st.file_uploader(
            "é€‰æ‹©DAU CSVæ–‡ä»¶",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶åæ ¼å¼: dau_æ¸ é“_æ—¥æœŸ.csv",
            key="dau_uploader"
        )
        
        if dau_files:
            st.success(f"å·²é€‰æ‹© {len(dau_files)} ä¸ªDAUæ–‡ä»¶")
            
            if st.button("ğŸš€ å¤„ç†DAUæ–‡ä»¶", type="primary", key="process_dau"):
                with st.spinner("æ­£åœ¨å¤„ç†DAUæ–‡ä»¶..."):
                    st.session_state.dau_results = process_dau_files(dau_files)
                
                if st.session_state.dau_results:
                    st.success("âœ… DAUæ–‡ä»¶å¤„ç†å®Œæˆ!")
    
    # ç•™å­˜æ–‡ä»¶å¤„ç†æ ‡ç­¾é¡µ
    with tab2:
        st.subheader("ğŸ“ ä¸Šä¼ ç•™å­˜æ–‡ä»¶")
        retention_files = st.file_uploader(
            "é€‰æ‹©ç•™å­˜CSVæ–‡ä»¶",
            type=['csv'],
            accept_multiple_files=True,
            help="æ–‡ä»¶åæ ¼å¼: retention_æ¸ é“.csv",
            key="retention_uploader"
        )
        
        if retention_files:
            st.success(f"å·²é€‰æ‹© {len(retention_files)} ä¸ªç•™å­˜æ–‡ä»¶")
            
            if st.button("ğŸš€ å¤„ç†ç•™å­˜æ–‡ä»¶", type="primary", key="process_retention"):
                with st.spinner("æ­£åœ¨å¤„ç†ç•™å­˜æ–‡ä»¶..."):
                    st.session_state.retention_results = process_retention_files(retention_files)
                
                if st.session_state.retention_results:
                    st.success("âœ… ç•™å­˜æ–‡ä»¶å¤„ç†å®Œæˆ!")
    
    # åº•è¡¨æ—¥æœŸåˆ é™¤æ ‡ç­¾é¡µ
    with tab3:
        delete_excel_by_date_interface()
    
    # æ•°æ®æ ¡éªŒæ ‡ç­¾é¡µ
    with tab4:
        validate_data_interface()
    
    # å¦‚æœæœ‰DAUæˆ–ç•™å­˜å¤„ç†ç»“æœï¼Œæ˜¾ç¤ºæ•°æ®é¢„è§ˆå’Œä¸‹è½½é€‰é¡¹
    if st.session_state.dau_results or st.session_state.retention_results:
        st.markdown("---")
        st.subheader("ğŸ“Š æ–‡ä»¶å¤„ç†ç»“æœ")
        
        # åˆ›å»ºç»“æœæ ‡ç­¾é¡µ
        result_tabs = []
        if st.session_state.dau_results:
            result_tabs.append("ğŸ“ˆ DAUæ•°æ®")
        if st.session_state.retention_results:
            result_tabs.append("ğŸ”„ ç•™å­˜æ•°æ®")
        
        if result_tabs:
            tabs = st.tabs(result_tabs)
            tab_index = 0
            
            # DAUç»“æœæ˜¾ç¤º
            if st.session_state.dau_results:
                with tabs[tab_index]:
                    dau_data = st.session_state.dau_results
                    
                    # åˆ›å»ºæ•´åˆçš„DAUæ•°æ®
                    integrated_dau = create_integrated_dau(dau_data)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¤„ç†æ¸ é“æ•°", len(dau_data))
                    with col2:
                        total_rows = sum(len(df) for df in dau_data.values())
                        st.metric("æ€»æ•°æ®è¡Œæ•°", total_rows)
                    with col3:
                        if not integrated_dau.empty:
                            st.metric("æ•´åˆåè¡Œæ•°", len(integrated_dau))
                    
                    # æ•°æ®é¢„è§ˆ
                    preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯DAUæ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in dau_data.keys()])
                    
                    # ä¸‰ç«¯æ±‡æ€»é¢„è§ˆ
                    with preview_tabs[0]:
                        if not integrated_dau.empty:
                            st.dataframe(integrated_dau.head(10), use_container_width=True)
                        else:
                            st.error("æ— æ³•åˆ›å»ºä¸‰ç«¯DAUæ±‡æ€»æ•°æ®")
                    
                    # å„æ¸ é“é¢„è§ˆ
                    for i, (channel, df) in enumerate(dau_data.items()):
                        with preview_tabs[i + 1]:
                            st.dataframe(df.head(10), use_container_width=True)
                
                tab_index += 1
            
            # ç•™å­˜ç»“æœæ˜¾ç¤º
            if st.session_state.retention_results:
                with tabs[tab_index]:
                    retention_data = st.session_state.retention_results
                    
                    # åˆ›å»ºæ•´åˆçš„ç•™å­˜æ•°æ®
                    integrated_retention = create_integrated_retention(retention_data)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å¤„ç†æ¸ é“æ•°", len(retention_data))
                    with col2:
                        total_rows = sum(len(df) for df in retention_data.values())
                        st.metric("æ€»æ•°æ®è¡Œæ•°", total_rows)
                    with col3:
                        if not integrated_retention.empty:
                            st.metric("æ•´åˆåè¡Œæ•°", len(integrated_retention))
                    
                    # æ•°æ®é¢„è§ˆ
                    preview_tabs = st.tabs(["ğŸ¯ ä¸‰ç«¯ç•™å­˜æ±‡æ€»"] + [f"{ch.upper()}æ¸ é“" for ch in retention_data.keys()])
                    
                    # ä¸‰ç«¯æ±‡æ€»é¢„è§ˆ
                    with preview_tabs[0]:
                        if not integrated_retention.empty:
                            st.dataframe(integrated_retention.head(10), use_container_width=True)
                        else:
                            st.error("æ— æ³•åˆ›å»ºä¸‰ç«¯ç•™å­˜æ±‡æ€»æ•°æ®")
                    
                    # å„æ¸ é“é¢„è§ˆ
                    for i, (channel, df) in enumerate(retention_data.items()):
                        with preview_tabs[i + 1]:
                            st.dataframe(df.head(10), use_container_width=True)
        
        # ä¸‹è½½åŒºåŸŸ
        st.markdown("---")
        st.subheader("ğŸ’¾ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶")
        
        today = datetime.datetime.now().strftime("%m.%d")
        
        # ä¸»è¦ä¸‹è½½é€‰é¡¹
        st.markdown("### ğŸ¯ **æ±‡æ€»æ–‡ä»¶ä¸‹è½½**")
        
        download_cols = st.columns(2)
        
        # DAUæ±‡æ€»ä¸‹è½½
        if st.session_state.dau_results:
            with download_cols[0]:
                integrated_dau = create_integrated_dau(st.session_state.dau_results)
                if not integrated_dau.empty:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = integrated_dau.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“ˆ ä¸‹è½½ä¸‰ç«¯DAUæ±‡æ€»æ–‡ä»¶",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ä¸‰ç«¯dauæ±‡æ€».csv",
                        mime="text/csv",
                        type="primary"
                    )
                    st.success(f"âœ… {len(integrated_dau)} è¡ŒDAUæ•°æ®")
                else:
                    st.error("âŒ DAUæ±‡æ€»æ•°æ®ç”Ÿæˆå¤±è´¥")
        
        # ç•™å­˜æ±‡æ€»ä¸‹è½½
        if st.session_state.retention_results:
            with download_cols[1]:
                integrated_retention = create_integrated_retention(st.session_state.retention_results)
                if not integrated_retention.empty:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = integrated_retention.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ”„ ä¸‹è½½ä¸‰ç«¯ç•™å­˜æ±‡æ€»æ–‡ä»¶",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ä¸‰ç«¯ç•™å­˜æ±‡æ€».csv",
                        mime="text/csv",
                        type="primary"
                    )
                    st.success(f"âœ… {len(integrated_retention)} è¡Œç•™å­˜æ•°æ®")
                else:
                    st.error("âŒ ç•™å­˜æ±‡æ€»æ•°æ®ç”Ÿæˆå¤±è´¥")
        
        # åˆ†æ¸ é“æ–‡ä»¶ä¸‹è½½
        st.markdown("### ğŸ“ **åˆ†æ¸ é“æ–‡ä»¶ä¸‹è½½**")
        
        # DAUåˆ†æ¸ é“ä¸‹è½½
        if st.session_state.dau_results:
            st.markdown("**DAUåˆ†æ¸ é“æ–‡ä»¶:**")
            dau_cols = st.columns(len(st.session_state.dau_results))
            for i, (channel, df) in enumerate(st.session_state.dau_results.items()):
                with dau_cols[i]:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ“ˆ DAU-{channel.upper()}",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} dauæ±‡æ€»_{channel}.csv",
                        mime="text/csv",
                        key=f"dau_{channel}"
                    )
                    st.text(f"{len(df)} è¡Œæ•°æ®")
        
        # ç•™å­˜åˆ†æ¸ é“ä¸‹è½½
        if st.session_state.retention_results:
            st.markdown("**ç•™å­˜åˆ†æ¸ é“æ–‡ä»¶:**")
            retention_cols = st.columns(len(st.session_state.retention_results))
            for i, (channel, df) in enumerate(st.session_state.retention_results.items()):
                with retention_cols[i]:
                    # ä½¿ç”¨UTF-8 BOMç¼–ç ç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label=f"ğŸ”„ ç•™å­˜-{channel.upper()}",
                        data=csv_data.encode('utf-8-sig'),
                        file_name=f"{today} ç•™å­˜_{channel}.csv",
                        mime="text/csv",
                        key=f"retention_{channel}"
                    )
                    st.text(f"{len(df)} è¡Œæ•°æ®")
    
    else:
        st.info("ğŸ‘† è¯·é€‰æ‹©ç›¸åº”çš„æ ‡ç­¾é¡µå¼€å§‹å¤„ç†æ•°æ®")
    
    # ========== ä½¿ç”¨è¯´æ˜å’Œä¿¡æ¯åŒºåŸŸ ==========
    st.markdown("---")
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“‹ ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ### ğŸ¯ åŠŸèƒ½æ¦‚è¿°
        1. **DAUæ–‡ä»¶åˆå¹¶**: å¤„ç†å¤šä¸ªDAU CSVæ–‡ä»¶ï¼ŒæŒ‰æ¸ é“åˆ†ç»„åˆå¹¶
        2. **ç•™å­˜ç‡è®¡ç®—**: å¤„ç†ç•™å­˜æ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨è®¡ç®—å„å¤©ç•™å­˜ç‡
        3. **åº•è¡¨æ—¥æœŸåˆ é™¤**: åˆ é™¤Excelåº•è¡¨ä¸­æŒ‡å®šæ—¥æœŸåŠä¹‹åçš„æ•°æ®
        4. **æ•°æ®æ ¡éªŒ**: å¯¹æ¯”åˆ†æåº•è¡¨æ•°æ®ä¸retention_all.csvæ•°æ®çš„ä¸€è‡´æ€§
        
        ### ğŸ“ æ–‡ä»¶è¦æ±‚
        **DAUæ–‡ä»¶å‘½å**: `dau_æ¸ é“_æ—¥æœŸ.csv` (ä¾‹å¦‚: `dau_mvp_3.17.csv`)
        - æ”¯æŒæ¸ é“: mvp, and, ios
        
        **ç•™å­˜æ–‡ä»¶å‘½å**: 
        - `retention_ios.csv` (iOSæ¸ é“)
        - `retention_ios_formal.csv` (iOSæ­£å¼æ¸ é“)
        - `retention_mvp.csv` (MVPæ¸ é“)
        - `retention_and.csv` (Androidæ¸ é“)
        
        **åº•è¡¨æ–‡ä»¶**: Excelæ ¼å¼ï¼ŒåŒ…å«"ä¸‰ç«¯DAU"å’Œ"ä¸‰ç«¯ç•™å­˜"å·¥ä½œè¡¨
        
        ### ğŸ“¤ è¾“å‡ºæ–‡ä»¶
        - **ä¸‰ç«¯DAUæ±‡æ€»æ–‡ä»¶**: åŒ…å«æ‰€æœ‰æ¸ é“DAUæ•°æ®
        - **ä¸‰ç«¯ç•™å­˜æ±‡æ€»æ–‡ä»¶**: åŒ…å«æ‰€æœ‰æ¸ é“ç•™å­˜æ•°æ®
        - **å„æ¸ é“å•ç‹¬æ–‡ä»¶**: DAUå’Œç•™å­˜çš„åˆ†æ¸ é“æ–‡ä»¶
        - **å¤„ç†ååº•è¡¨**: åˆ é™¤æŒ‡å®šæ—¥æœŸåçš„Excelæ–‡ä»¶
        - **æ•°æ®æ ¡éªŒæŠ¥å‘Š**: æ•°æ®ä¸€è‡´æ€§åˆ†æç»“æœ
        """)
    
    # ç¯å¢ƒè¦æ±‚
    with st.expander("ğŸ”§ ç¯å¢ƒè¦æ±‚", expanded=False):
        st.markdown("""
        ### ç³»ç»Ÿè¦æ±‚
        - **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
        - **æ“ä½œç³»ç»Ÿ**: Windows, macOS, Linux
        
        ### ä¾èµ–åŒ…è¦æ±‚
        - **streamlit**: >= 1.28.0 (Webåº”ç”¨æ¡†æ¶)
        - **pandas**: >= 1.5.0 (æ•°æ®å¤„ç†)
        - **numpy**: >= 1.20.0 (æ•°å€¼è®¡ç®—)
        - **openpyxl**: >= 3.0.0 (Excelæ–‡ä»¶å¤„ç†)
        
        ### å®‰è£…æ–¹æ³•
        ```bash
        # å•ç‹¬å®‰è£…
        pip install streamlit pandas numpy openpyxl
        
        # æˆ–è€…ä½¿ç”¨requirements.txt
        pip install -r requirements.txt
        ```
        
        ### è¿è¡Œåº”ç”¨
        ```bash
        streamlit run app.py
        ```
        """)
    
    # é¡µè„šä¿¡æ¯
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 20px;'>
            <h4>ğŸ“Š å®Œæ•´æ•°æ®å¤„ç†å·¥å…· - é›†æˆç‰ˆ</h4>
            <p><strong>åŠŸèƒ½æ¨¡å—:</strong> DAUåˆå¹¶ + ç•™å­˜è®¡ç®— + åº•è¡¨ç®¡ç† + æ•°æ®æ ¡éªŒ</p>
            <p style='margin-top: 20px; font-size: 14px; background-color: #f8f9fa; padding: 10px; border-radius: 5px;'>
                <strong>ğŸš€ å¿«é€Ÿå®‰è£…ä¾èµ–åŒ…:</strong><br>
                <code style='background-color: #e9ecef; padding: 2px 6px; border-radius: 3px; color: #495057;'>
                pip install streamlit pandas numpy openpyxl
                </code>
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
