import streamlit as st
import pandas as pd
import datetime
import re
import io
import zipfile
from typing import Dict, List, Optional

def convert_date_to_sortable(date_str: str) -> str:
    """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯æ’åºçš„æ ¼å¼"""
    try:
        parts = date_str.split('/')
        if len(parts) == 3:
            year, month, day = parts
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{year}{month}{day}"
    except:
        pass
    return date_str

def process_uploaded_files(uploaded_files) -> Optional[Dict[str, pd.DataFrame]]:
    """å¤„ç†ä¸Šä¼ çš„CSVæ–‡ä»¶"""
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # å­˜å‚¨æ¯ä¸ªæ¸ é“çš„DataFrameåˆ—è¡¨
    channel_dfs = {'mvp': [], 'and': [], 'ios': []}
    standard_columns = {'mvp': None, 'and': None, 'ios': None}
    
    processed_files = 0
    total_files = len(uploaded_files)
    
    # åˆ›å»ºè¯¦ç»†æ—¥å¿—å®¹å™¨
    log_container = st.expander("å¤„ç†è¯¦æƒ…", expanded=False)
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name} ({i+1}/{total_files})")
            
            filename = uploaded_file.name
            
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«"dau"
            if "dau" not in filename.lower():
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åä¸åŒ…å«'dau'")
                continue
            
            # ä»æ–‡ä»¶åä¸­æå–æ¸ é“ä¿¡æ¯
            if len(filename) > 7 and filename.startswith("dau_"):
                channel = filename[4:7]  # è·å–æ¸ é“å (mvp, and, ios)
                if channel not in channel_dfs:
                    log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ— æ³•è¯†åˆ«çš„æ¸ é“ '{channel}'")
                    continue
            else:
                log_container.warning(f"è·³è¿‡æ–‡ä»¶ {filename}: æ–‡ä»¶åæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                continue
            
            # è¯»å–CSVæ–‡ä»¶
            try:
                # å°è¯•ä¸åŒçš„ç¼–ç 
                content = uploaded_file.getvalue()
                try:
                    df = pd.read_csv(io.StringIO(content.decode('utf-8')), 
                                   na_values=[''], keep_default_na=False)
                except UnicodeDecodeError:
                    df = pd.read_csv(io.StringIO(content.decode('latin1')), 
                                   na_values=[''], keep_default_na=False)
                
                log_container.success(f"æˆåŠŸè¯»å– {filename}, å½¢çŠ¶: {df.shape}")
                
            except Exception as e:
                log_container.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
                continue
            
            # éªŒè¯æ•°æ®ä¸ä¸ºç©º
            if df.empty:
                log_container.warning(f"æ–‡ä»¶ {filename} ä¸åŒ…å«æ•°æ®ï¼Œå·²è·³è¿‡")
                continue
            
            # åˆ é™¤æŒ‡å®šçš„ä¸‰åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            columns_to_drop = ['Total Conversions', 'Re-attribution', 'Re-engagement']
            original_cols = df.columns.tolist()
            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')
            removed_cols = [col for col in columns_to_drop if col in original_cols]
            if removed_cols:
                log_container.info(f"å·²åˆ é™¤åˆ—: {', '.join(removed_cols)}")
            
            # ä»æ–‡ä»¶åæå–æ—¥æœŸéƒ¨åˆ†å¹¶æ ¼å¼åŒ–
            try:
                date_part = filename.split('_')[-1].replace('.csv', '')
                match = re.search(r'(\d+)\.(\d+)', date_part)
                if match:
                    month, day = match.groups()
                    formatted_date = f"2025/{month}/{day}"
                else:
                    formatted_date = date_part
            except:
                formatted_date = "2025/1/1"
                log_container.warning(f"æ— æ³•ä»æ–‡ä»¶å {filename} æå–æ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            # æ·»åŠ æ—¥æœŸåˆ—åˆ°DataFrameçš„æœ€å‰é¢
            df.insert(0, 'date', formatted_date)
            
            # å¦‚æœæ˜¯iOSæ¸ é“ä¸”å‘ç°æœ‰é—®é¢˜çš„åˆ—
            if channel == 'ios' and 'Average eCPIUS$2.50' in df.columns:
                df = df.drop(columns=['Average eCPIUS$2.50'])
                log_container.info(f"ç§»é™¤iOSä¸­çš„é—®é¢˜åˆ—: 'Average eCPIUS$2.50'")
            
            # åˆ—æ ‡å‡†åŒ–
            if standard_columns[channel] is None:
                standard_columns[channel] = df.columns.tolist()
                log_container.info(f"è®¾ç½® {channel} æ¸ é“çš„æ ‡å‡†åˆ—")
            else:
                current_cols = df.columns.tolist()
                if current_cols != standard_columns[channel]:
                    # è°ƒæ•´åˆ—åä»¥åŒ¹é…æ ‡å‡†åˆ—
                    missing_cols = [col for col in standard_columns[channel] if col not in current_cols]
                    extra_cols = [col for col in current_cols if col not in standard_columns[channel]]
                    
                    if missing_cols:
                        for col in missing_cols:
                            df[col] = 'N/A'
                        log_container.info(f"æ·»åŠ ç¼ºå°‘çš„åˆ—: {', '.join(missing_cols)}")
                    
                    if extra_cols:
                        df = df.drop(columns=extra_cols)
                        log_container.info(f"ç§»é™¤å¤šä½™çš„åˆ—: {', '.join(extra_cols)}")
                    
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df[standard_columns[channel]]
            
            # å°†ç©ºå€¼è½¬æ¢ä¸º"N/A"
            df = df.fillna('N/A')
            
            # æ·»åŠ åˆ°å¯¹åº”æ¸ é“
            channel_dfs[channel].append(df)
            processed_files += 1
            
        except Exception as e:
            log_container.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    # å®Œæˆè¿›åº¦
    progress_bar.progress(1.0)
    status_text.text(f"å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶")
    
    if processed_files == 0:
        st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
        return None
    
    # åˆå¹¶æ•°æ®å¹¶æŒ‰æ—¥æœŸæ’åº
    merged_by_channel = {}
    
    for channel, df_list in channel_dfs.items():
        if df_list:
            st.info(f"æ­£åœ¨åˆå¹¶æ¸ é“ {channel} çš„ {len(df_list)} ä¸ªæ–‡ä»¶...")
            
            # ç¡®ä¿æ‰€æœ‰DataFrameéƒ½æœ‰ç›¸åŒçš„åˆ—
            if len(df_list) > 1:
                standard_cols = list(df_list[0].columns)
                for i, df in enumerate(df_list):
                    if set(df.columns) != set(standard_cols):
                        missing = [col for col in standard_cols if col not in df.columns]
                        extra = [col for col in df.columns if col not in standard_cols]
                        
                        for col in missing:
                            df[col] = 'N/A'
                        if extra:
                            df = df.drop(columns=extra)
                        df = df[standard_cols]
                        df_list[i] = df
            
            # åˆå¹¶è¯¥æ¸ é“çš„æ‰€æœ‰DataFrame
            merged_df = pd.concat(df_list, ignore_index=True)
            
            # æŒ‰æ—¥æœŸæ’åº
            try:
                merged_df['sort_key'] = merged_df['date'].apply(convert_date_to_sortable)
                merged_df = merged_df.sort_values(by='sort_key')
                merged_df = merged_df.drop(columns=['sort_key'])
            except Exception as e:
                st.warning(f"æ¸ é“ {channel} æ’åºæ—¶å‡ºé”™: {str(e)}")
            
            # ç¡®ä¿ç©ºå€¼è½¬æ¢ä¸ºN/A
            merged_df = merged_df.fillna('N/A')
            
            # iOSæ¸ é“ç‰¹æ®Šå¤„ç†
            if channel == 'ios':
                expected_columns = ['date', 'Country', 'Impressions', 'Clicks', 'Installs', 'Conversion Rate',
                                  'Activity Sessions', 'Cost', 'Activity Revenue', 'Average eCPIUS$2.31',
                                  'Average DAU', 'Average MAU', 'Average DAU/MAU Rate', 'ARPDAU']
                
                extra_columns = [col for col in merged_df.columns if col not in expected_columns]
                if extra_columns:
                    merged_df = merged_df.drop(columns=extra_columns)
                
                missing_columns = [col for col in expected_columns if col not in merged_df.columns]
                if missing_columns:
                    for col in missing_columns:
                        merged_df[col] = 'N/A'
                
                merged_df = merged_df[expected_columns]
            
            merged_by_channel[channel] = merged_df
    
    return merged_by_channel

def create_download_zip(merged_data: Dict[str, pd.DataFrame]) -> bytes:
    """åˆ›å»ºåŒ…å«æ‰€æœ‰åˆå¹¶æ–‡ä»¶çš„ZIPæ–‡ä»¶"""
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        today = datetime.datetime.now().strftime("%m.%d")
        
        for channel, df in merged_data.items():
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False, encoding='utf-8')
            csv_content = csv_buffer.getvalue().encode('utf-8')
            
            filename = f"{today} dauæ±‡æ€»_{channel}.csv"
            zip_file.writestr(filename, csv_content)
    
    zip_buffer.seek(0)
    return zip_buffer.getvalue()

def main():
    st.set_page_config(
        page_title="CSVæ–‡ä»¶åˆå¹¶å·¥å…·",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š CSVæ–‡ä»¶åˆå¹¶å·¥å…·")
    st.markdown("---")
    
    # è¯´æ˜æ–‡æ¡£
    with st.expander("ğŸ“‹ ä½¿ç”¨è¯´æ˜", expanded=True):
        st.markdown("""
        ### åŠŸèƒ½è¯´æ˜
        - åˆå¹¶å¤šä¸ªDAUç›¸å…³çš„CSVæ–‡ä»¶
        - æŒ‰æ¸ é“åˆ†ç»„ (mvp, and, ios)
        - è‡ªåŠ¨å¤„ç†æ—¥æœŸæ ¼å¼å’Œæ•°æ®æ¸…æ´—
        - ç”ŸæˆæŒ‰æ¸ é“åˆ†ç»„çš„åˆå¹¶æ–‡ä»¶
        
        ### æ–‡ä»¶å‘½åè¦æ±‚
        - æ–‡ä»¶åå¿…é¡»åŒ…å« "dau"
        - æ–‡ä»¶åæ ¼å¼åº”ä¸º: `dau_æ¸ é“_æ—¥æœŸ.csv` (ä¾‹å¦‚: `dau_mvp_3.17.csv`)
        - æ”¯æŒçš„æ¸ é“: mvp, and, ios
        
        ### æ•°æ®å¤„ç†
        - è‡ªåŠ¨åˆ é™¤ 'Total Conversions', 'Re-attribution', 'Re-engagement' åˆ—
        - æ·»åŠ æ—¥æœŸåˆ—å¹¶æŒ‰æ—¥æœŸæ’åº
        - ç»Ÿä¸€åˆ—æ ¼å¼å’Œå¤„ç†ç¼ºå¤±å€¼
        """)
    
    # æ–‡ä»¶ä¸Šä¼ 
    st.subheader("ğŸ“ ä¸Šä¼ CSVæ–‡ä»¶")
    uploaded_files = st.file_uploader(
        "é€‰æ‹©è¦åˆå¹¶çš„CSVæ–‡ä»¶",
        type=['csv'],
        accept_multiple_files=True,
        help="å¯ä»¥åŒæ—¶é€‰æ‹©å¤šä¸ªCSVæ–‡ä»¶"
    )
    
    if uploaded_files:
        st.success(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        with st.expander("æŸ¥çœ‹ä¸Šä¼ çš„æ–‡ä»¶"):
            for file in uploaded_files:
                st.text(f"ğŸ“„ {file.name} ({file.size} bytes)")
        
        # å¤„ç†æŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary"):
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                merged_data = process_uploaded_files(uploaded_files)
            
            if merged_data:
                st.success("âœ… æ–‡ä»¶å¤„ç†å®Œæˆ!")
                
                # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
                st.subheader("ğŸ“ˆ å¤„ç†ç»“æœæ‘˜è¦")
                
                col1, col2, col3 = st.columns(3)
                
                for i, (channel, df) in enumerate(merged_data.items()):
                    with [col1, col2, col3][i]:
                        st.metric(
                            label=f"æ¸ é“ {channel.upper()}",
                            value=f"{len(df)} è¡Œæ•°æ®"
                        )
                        
                        # æ˜¾ç¤ºæ—¥æœŸèŒƒå›´
                        dates = sorted(df['date'].unique(), key=convert_date_to_sortable)
                        if dates:
                            st.text(f"æ—¥æœŸèŒƒå›´: {dates[0]} ~ {dates[-1]}")
                
                # æ•°æ®é¢„è§ˆ
                st.subheader("ğŸ‘€ æ•°æ®é¢„è§ˆ")
                
                tab_names = [f"æ¸ é“ {channel.upper()}" for channel in merged_data.keys()]
                tabs = st.tabs(tab_names)
                
                for tab, (channel, df) in zip(tabs, merged_data.items()):
                    with tab:
                        st.dataframe(df.head(10), use_container_width=True)
                        st.text(f"æ˜¾ç¤ºå‰10è¡Œï¼Œæ€»å…± {len(df)} è¡Œ")
                
                # ä¸‹è½½åŒºåŸŸ
                st.subheader("ğŸ’¾ ä¸‹è½½åˆå¹¶åçš„æ–‡ä»¶")
                
                # åˆ›å»ºZIPæ–‡ä»¶
                zip_data = create_download_zip(merged_data)
                today = datetime.datetime.now().strftime("%m.%d")
                
                st.download_button(
                    label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰åˆå¹¶æ–‡ä»¶ (ZIP)",
                    data=zip_data,
                    file_name=f"{today} dauæ±‡æ€»_æ‰€æœ‰æ¸ é“.zip",
                    mime="application/zip"
                )
                
                # å•ç‹¬ä¸‹è½½æ¯ä¸ªæ¸ é“çš„æ–‡ä»¶
                st.markdown("**æˆ–è€…å•ç‹¬ä¸‹è½½å„æ¸ é“æ–‡ä»¶:**")
                
                cols = st.columns(len(merged_data))
                for col, (channel, df) in zip(cols, merged_data.items()):
                    with col:
                        csv_data = df.to_csv(index=False, encoding='utf-8')
                        filename = f"{today} dauæ±‡æ€»_{channel}.csv"
                        
                        st.download_button(
                            label=f"ğŸ“„ ä¸‹è½½ {channel.upper()}",
                            data=csv_data.encode('utf-8'),
                            file_name=filename,
                            mime="text/csv"
                        )
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ CSVæ–‡ä»¶å¼€å§‹å¤„ç†")
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            <p>CSVæ–‡ä»¶åˆå¹¶å·¥å…· | æ”¯æŒDAUæ•°æ®å¤„ç†å’Œæ¸ é“åˆ†ç»„</p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
